
@article{aujol_fista_2023,
	title = {{FISTA} is an automatic geometrically optimized algorithm for strongly convex functions},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-023-01960-6},
	doi = {10.1007/s10107-023-01960-6},
	abstract = {In this work, we are interested in the famous FISTA algorithm. We show that FISTA is an automatic geometrically optimized algorithm for functions satisfying a quadratic growth assumption. This explains why FISTA works better than the standard Forward-Backward algorithm (FB) in such a case, although FISTA is known to have a polynomial asymptotic convergence rate while FB is exponential. We provide a simple rule to tune the \$\${\textbackslash}alpha \$\$parameter within the FISTA algorithm to reach an \$\${\textbackslash}varepsilon \$\$-solution with an optimal number of iterations. These new results highlight the efficiency of FISTA algorithms, and they rely on new non asymptotic bounds for FISTA.},
	language = {en},
	urldate = {2023-10-09},
	journal = {Mathematical Programming},
	author = {Aujol, J.-F. and Dossal, Ch. and Rondepierre, A.},
	month = apr,
	year = {2023},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/JP7BSENL/Aujol et al. - 2023 - FISTA is an automatic geometrically optimized algo.pdf:application/pdf},
}

@article{aujol_fista_2022,
	title = {{FISTA} restart using an automatic estimation of the growth parameter},
	url = {https://hal.science/hal-03153525},
	abstract = {In this paper, we propose a restart scheme for FISTA (Fast Iterative Shrinking-Threshold Algorithm). This method which is a generalization of Nesterov's accelerated gradient algorithm is widely used in the field of large convex optimization problems and it provides fast convergence results under a strong convexity assumption. These convergence rates can be extended for weaker hypotheses such as the {\textbackslash}L\{\}ojasiewicz property but it requires prior knowledge on the function of interest. In particular, most of the schemes providing a fast convergence for non-strongly convex functions satisfying a quadratic growth condition involve the growth parameter which is generally not known. Recent works show that restarting FISTA could ensure a fast convergence for this class of functions without requiring any knowledge on the growth parameter. We improve these restart schemes by providing a better asymptotical convergence rate and by requiring a lower computation cost. We present numerical results emphasizing the efficiency of this method.},
	urldate = {2023-10-09},
	author = {Aujol, Jean-François and Dossal, Charles H and Labarrière, Hippolyte and Rondepierre, Aude},
	month = may,
	year = {2022},
	file = {HAL PDF Full Text:/Users/hongdali/Zotero/storage/VQV7T5EP/Aujol et al. - 2022 - FISTA restart using an automatic estimation of the.pdf:application/pdf},
}

@article{noel_nesterovs_nodate,
	title = {Nesterov's {Method} for {Convex} {Optimization}},
	volume = {65},
	url = {https://epubs-siam-org.eu1.proxy.openathens.net/doi/epdf/10.1137/21M1390037},
	doi = {10.1137/21M1390037},
	abstract = {While Nesterov's algorithm for computing the minimum of a convex function is now over forty years old, it is rarely presented in texts for a first course in optimization. This is unfortunate since for many problems this algorithm is superior to the ubiquitous steepest descent algorithm, and it is equally simple to implement. This article presents an elementary analysis of Nesterov's algorithm that parallels that of steepest descent. It is envisioned that this presentation of Nesterov's algorithm could easily be covered in a few lectures following the introductory material on convex functions and steepest descent included in every course on optimization.},
	language = {en},
	number = {2},
	urldate = {2023-10-09},
	journal = {SIAM Review},
	author = {Noel, Walkington},
	doi = {10.1137/21M1390037},
	pages = {539--562},
	file = {Nesterov's Method for Convex Optimization.pdf:/Users/hongdali/Zotero/storage/BZC6TFHX/Nesterov's Method for Convex Optimization.pdf:application/pdf;Snapshot:/Users/hongdali/Zotero/storage/M8MDL6E3/21M1390037.html:text/html},
}

@misc{necoara_linear_2015,
	title = {Linear convergence of first order methods for non-strongly convex optimization (draft)},
	url = {https://arxiv.org/abs/1504.06298v4},
	abstract = {The standard assumption for proving linear convergence of first order methods for smooth convex optimization is the strong convexity of the objective function, an assumption which does not hold for many practical applications. In this paper, we derive linear convergence rates of several first order methods for solving smooth non-strongly convex constrained optimization problems, i.e. involving an objective function with a Lipschitz continuous gradient that satisfies some relaxed strong convexity condition. In particular, in the case of smooth constrained convex optimization, we provide several relaxations of the strong convexity conditions and prove that they are sufficient for getting linear convergence for several first order methods such as projected gradient, fast gradient and feasible descent methods. We also provide examples of functional classes that satisfy our proposed relaxations of strong convexity conditions. Finally, we show that the proposed relaxed strong convexity conditions cover important applications ranging from solving linear systems, Linear Programming, and dual formulations of linearly constrained convex problems.},
	language = {en},
	urldate = {2023-10-09},
	author = {Necoara, I. and Nesterov, Yu and Glineur, F.},
	month = apr,
	year = {2015},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/5Q6738AN/Necoara et al. - 2015 - Linear convergence of first order methods for non-.pdf:application/pdf},
}

@article{aujol_parameter-free_2023,
	title = {Parameter-{Free} {FISTA} by {Adaptive} {Restart} and {Backtracking}},
	url = {https://arxiv.org/abs/2307.14323v1},
	abstract = {We consider a combined restarting and adaptive backtracking strategy for the popular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for accelerating the convergence speed of large-scale structured convex optimization problems. Several variants of FISTA enjoy a provable linear convergence rate for the function values \$F(x\_n)\$ of the form \${\textbackslash}mathcal\{O\}( e{\textasciicircum}\{-K{\textbackslash}sqrt\{{\textbackslash}mu/L\}{\textasciitilde}n\})\$ under the prior knowledge of problem conditioning, i.e. of the ratio between the ({\textbackslash}L ojasiewicz) parameter \${\textbackslash}mu\$ determining the growth of the objective function and the Lipschitz constant \$L\$ of its smooth component. These parameters are nonetheless hard to estimate in many practical cases. Recent works address the problem by estimating either parameter via suitable adaptive strategies. In our work both parameters can be estimated at the same time by means of an algorithmic restarting scheme where, at each restart, a non-monotone estimation of \$L\$ is performed. For this scheme, theoretical convergence results are proved, showing that a \${\textbackslash}mathcal\{O\}( e{\textasciicircum}\{-K{\textbackslash}sqrt\{{\textbackslash}mu/L\}n\})\$ convergence speed can still be achieved along with quantitative estimates of the conditioning. The resulting Free-FISTA algorithm is therefore parameter-free. Several numerical results are reported to confirm the practical interest of its use in many exemplar problems.},
	language = {en},
	urldate = {2023-10-09},
	journal = {arXiv.org},
	author = {Aujol, Jean-François and Calatroni, Luca and Dossal, Charles and Labarrière, Hippolyte and Rondepierre, Aude},
	month = jul,
	year = {2023},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/3YBXJH4F/Aujol et al. - 2023 - Parameter-Free FISTA by Adaptive Restart and Backt.pdf:application/pdf},
}

@article{su_differential_2015,
	title = {A {Differential} {Equation} for {Modeling} {Nesterov}'s {Accelerated} {Gradient} {Method}: {Theory} and {Insights}},
	shorttitle = {A {Differential} {Equation} for {Modeling} {Nesterov}'s {Accelerated} {Gradient} {Method}},
	url = {https://arxiv.org/abs/1503.01243v2},
	abstract = {We derive a second-order ordinary differential equation (ODE) which is the limit of Nesterov's accelerated gradient method. This ODE exhibits approximate equivalence to Nesterov's scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov's scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov's scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.},
	language = {en},
	urldate = {2023-10-09},
	journal = {arXiv.org},
	author = {Su, Weijie and Boyd, Stephen and Candes, Emmanuel J.},
	month = mar,
	year = {2015},
	keywords = {ODEs, dynamical system},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/ANWIF5NT/Su et al. - 2015 - A Differential Equation for Modeling Nesterov's Ac.pdf:application/pdf},
}

@misc{ang_heavy_nodate,
	address = {Mathe  ́matique et recherche op ́erationnelle UMONS, Belgium},
	title = {Heavy {Ball} {Method} on {Quadratic} {Problems}},
	url = {https://angms.science/doc/CVX/CVX_HBM.pdf},
	language = {en},
	author = {Ang, Andersen},
	keywords = {lecture notes},
	file = {Ang - Math´ematique et recherche op´erationnelle UMONS, .pdf:/Users/hongdali/Zotero/storage/YU63848N/Ang - Math´ematique et recherche op´erationnelle UMONS, .pdf:application/pdf},
}

@article{aujol_convergence_2022,
	title = {Convergence {Rates} of the {Heavy} {Ball} {Method} for {Quasi}-strongly {Convex} {Optimization}},
	url = {https://hal.science/hal-02545245/document},
	doi = {10.1137/21M1403990},
	abstract = {In this paper, we discuss the convex optimization problem over the fixed point set of a nonexpansive mapping. The main objective of the paper is to accelerate the hybrid steepest descent method for the problem. To this goal, we present a new iterative scheme that utilizes the conjugate gradient direction. Its convergence to the solution is guaranteed under certain assumptions. In order to demonstrate the effectiveness, performance, and convergence of our proposed algorithm, we present numerical comparisons of the algorithm with the existing algorithm.},
	urldate = {2023-10-11},
	journal = {HAL open science},
	author = {Aujol, J.-F. and Dossal, Ch. and Rondepierre, A.},
	month = sep,
	year = {2022},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {heavy-ball method, Lyapunov Function, ODEs},
	file = {Submitted Version:/Users/hongdali/Zotero/storage/LFFE3WXB/Aujol et al. - 2022 - Convergence Rates of the Heavy Ball Method for Qua.pdf:application/pdf},
}

@article{polyak_methods_1964,
	title = {Some methods of speeding up the convergence of iteration methods},
	volume = {4},
	issn = {00415553},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0041555364901375},
	doi = {10.1016/0041-5553(64)90137-5},
	language = {en},
	number = {5},
	urldate = {2023-10-11},
	journal = {USSR Computational Mathematics and Mathematical Physics},
	author = {Polyak, B.T.},
	month = jan,
	year = {1964},
	pages = {1--17},
	file = {Polyak - 1964 - Some methods of speeding up the convergence of ite.pdf:/Users/hongdali/Zotero/storage/LFZIZLCJ/Polyak - 1964 - Some methods of speeding up the convergence of ite.pdf:application/pdf},
}

@article{attouch_rate_2016,
	title = {The {Rate} of {Convergence} of {Nesterov}'s {Accelerated} {Forward}-{Backward} {Method} is {Actually} {Faster} {Than} \$1/k{\textasciicircum}2\$},
	volume = {26},
	issn = {1052-6234},
	url = {https://epubs.siam.org/doi/10.1137/15M1046095},
	doi = {10.1137/15M1046095},
	abstract = {In a Hilbert space \${\textbackslash}mathcal H\$, assuming \$({\textbackslash}alpha\_k)\$ a general sequence of nonnegative numbers, we analyze the convergence properties of the inertial forward-backward algorithm \$(IFB){\textbackslash}\{\vphantom{\}}{\textbackslash}begin\{array\}\{l\}  y\_k=x\_k+{\textbackslash}alpha\_k(x\_k-x\_\{k-1\}), x\_\{k+1\}=\{{\textbackslash}rm prox\}\_\{s{\textbackslash}Psi\}(y\_k-s{\textbackslash}nabla {\textbackslash}Phi(y\_k)) {\textbackslash}end\{array\},\$ where \${\textbackslash}Psi: {\textbackslash}mathcal H {\textbackslash}to {\textbackslash}mathbb R {\textbackslash}cup {\textbackslash}lbrace + {\textbackslash}infty {\textbackslash}rbrace \$ is a proper lower-semicontinuous convex function, and \${\textbackslash}Phi: {\textbackslash}mathcal H {\textbackslash}to {\textbackslash}mathbb R\$ is a differentiable convex function, whose gradient is Lipschitz continuous. Various options for the sequence \$({\textbackslash}alpha\_k)\$ are considered in the literature. Among them, the Nesterov choice leads to the FISTA algorithm and accelerates convergence from \${\textbackslash}mathcal\{O\}(1/k)\$ to \${\textbackslash}mathcal\{O\}(1/k{\textasciicircum}2)\$ for the values. Several variants are used to guarantee the convergence of the iterates or to improve the rate of convergence for the values. For the design of fast optimization methods, the tuning of the sequence \$({\textbackslash}alpha\_k)\$ is a subtle issue, which we deal with in this paper in general. We show that the convergence rate of the algorithm can be obtained simply by analyzing the sequence of positive real numbers \$({\textbackslash}alpha\_k)\$. In addition to the case \${\textbackslash}alpha\_k= 1 -{\textbackslash}frac\{{\textbackslash}alpha\}\{k\} \$ with \${\textbackslash}alpha{\textbackslash}geq 3\$, our results apply equally well to \${\textbackslash}alpha\_k = 1- {\textbackslash}frac\{{\textbackslash}alpha\}\{k{\textasciicircum}r\}\$, with an exponent \$0{\textless}r{\textless}1\$, and to Polyak's heavy ball method. Thus, we unify most of the existing results based on the accelerated gradient method of Nesterov. In the process, we improve some of them and discover new ones.},
	number = {3},
	urldate = {2023-10-11},
	journal = {SIAM Journal on Optimization},
	author = {Attouch, Hedy and Peypouquet, Juan},
	month = jan,
	year = {2016},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {1824--1834},
	file = {Submitted Version:/Users/hongdali/Zotero/storage/VIWDS354/Attouch and Peypouquet - 2016 - The Rate of Convergence of Nesterov's Accelerated .pdf:application/pdf},
}

@misc{yu_acceleration_nodate,
	title = {Acceleration {Course} {Notes} for {CS794} at {University} of {Waterloo}},
	url = {https://cs.uwaterloo.ca/~y328yu/mycourses/794/794-note-apg.pdf},
	urldate = {2023-10-11},
	author = {Yu, Yaoliang},
	keywords = {lecture notes},
	file = {794-note-apg.pdf:/Users/hongdali/Zotero/storage/8TGD3FNG/794-note-apg.pdf:application/pdf},
}

@article{necoara_linear_2019,
	title = {Linear convergence of first order methods for non-strongly convex optimization},
	volume = {175},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-018-1232-1},
	doi = {10.1007/s10107-018-1232-1},
	abstract = {The standard assumption for proving linear convergence of first order methods for smooth convex optimization is the strong convexity of the objective function, an assumption which does not hold for many practical applications. In this paper, we derive linear convergence rates of several first order methods for solving smooth non-strongly convex constrained optimization problems, i.e. involving an objective function with a Lipschitz continuous gradient that satisfies some relaxed strong convexity condition. In particular, in the case of smooth constrained convex optimization, we provide several relaxations of the strong convexity conditions and prove that they are sufficient for getting linear convergence for several first order methods such as projected gradient, fast gradient and feasible descent methods. We also provide examples of functional classes that satisfy our proposed relaxations of strong convexity conditions. Finally, we show that the proposed relaxed strong convexity conditions cover important applications ranging from solving linear systems, Linear Programming, and dual formulations of linearly constrained convex problems.},
	language = {en},
	number = {1},
	urldate = {2023-10-11},
	journal = {Mathematical Programming},
	author = {Necoara, I. and Nesterov, Yu. and Glineur, F.},
	month = may,
	year = {2019},
	keywords = {65K05, 90C06, 90C25},
	pages = {69--107},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/7X79PGLC/Necoara et al. - 2019 - Linear convergence of first order methods for non-.pdf:application/pdf},
}

@article{rudin_nonlinear_1992,
	title = {Nonlinear total variation based noise removal algorithms},
	volume = {60},
	issn = {0167-2789},
	url = {https://www.sciencedirect.com/science/article/pii/016727899290242F},
	doi = {10.1016/0167-2789(92)90242-F},
	abstract = {A constrained optimization type of numerical algorithm for removing noise from images is presented. The total variation of the image is minimized subject to constraints involving the statistics of the noise. The constraints are imposed using Lanrange multipliers. The solution is obtained using the gradient-projection method. This amounts to solving a time dependent partial differential equation on a manifold determined by the constraints. As t → ∞ the solution converges to a steady state which is the denoised image. The numerical algorithm is simple and relatively fast. The results appear to be state-of-the-art for very noisy images. The method is noninvasive, yielding sharp edges in the image. The technique could be interpreted as a first step of moving each level set of the image normal to itself with velocity equal to the curvature of the level set divided by the magnitude of the gradient of the image, and a second step which projects the image back onto the constraint set.},
	number = {1},
	urldate = {2023-10-11},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Rudin, Leonid I. and Osher, Stanley and Fatemi, Emad},
	month = nov,
	year = {1992},
	keywords = {total variation, noise removal},
	pages = {259--268},
	file = {Rudin et al. - 1992 - Nonlinear total variation based noise removal algo.pdf:/Users/hongdali/Zotero/storage/VIG4E6VH/Rudin et al. - 1992 - Nonlinear total variation based noise removal algo.pdf:application/pdf},
}

@incollection{nesterov_lecture_2018,
	address = {Cham},
	series = {Springer {Optimization} and {Its} {Applications}},
	title = {Lecture on {Convex} {Optimizations} {Chapter} 2, {Smooth} {Convex} {Optimization}},
	isbn = {978-3-319-91578-4},
	url = {https://doi.org/10.1007/978-3-319-91578-4_2},
	abstract = {In this chapter, we study the complexity of solving optimization problems formed by differentiable convex components. We start by establishing the main properties of such functions and deriving the lower complexity bounds, which are valid for all natural optimization methods. After that, we prove the worst-case performance guarantees for the Gradient Method. Since these bounds are quite far from the lower complexity bounds, we develop a special technique, based on the notion of estimating sequences, which allows us to justify the Fast Gradient Methods. These methods appear to be optimal for smooth convex problems. We also obtain performance guarantees for these methods targeting on generating points with small norm of the gradient. In order to treat problems with set constraints, we introduce the notion of a Gradient Mapping. This allows an automatic extension of methods for unconstrained minimization to the constrained case. In the last section, we consider methods for solving smooth optimization problems, defined by several functional components.},
	language = {en},
	urldate = {2023-10-11},
	booktitle = {Lectures on {Convex} {Optimization}},
	publisher = {Springer International Publishing},
	author = {Nesterov, Yurii},
	editor = {Nesterov, Yurii},
	year = {2018},
	doi = {10.1007/978-3-319-91578-4_2},
	pages = {59--137},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/5LJMV866/Nesterov - 2018 - Smooth Convex Optimization.pdf:application/pdf},
}

@book{nesterov_lectures_2018,
	address = {Cham},
	series = {Springer {Optimization} and {Its} {Applications}},
	title = {Lectures on {Convex} {Optimization}},
	volume = {137},
	isbn = {978-3-319-91577-7 978-3-319-91578-4},
	url = {http://link.springer.com/10.1007/978-3-319-91578-4},
	urldate = {2023-10-11},
	publisher = {Springer International Publishing},
	author = {Nesterov, Yurii},
	year = {2018},
	doi = {10.1007/978-3-319-91578-4},
	keywords = {90C51, 90C52, 90C60, algorithm analysis and problem complexity, complexity, complexity theory, Cubic Regularization of Newton Method, Fast Gradient Methods, graphs, Interior-Point Methods, mathematical programming, MSC 2010 49M15, 49M29, 49N15, 65K05, 65K10, 90C25, 90C30, 90C46, optimization, Optimization in Relative Scale, Self-Concordant Functions, Smoothing Technique},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/HSCCPYL9/Nesterov - 2018 - Lectures on Convex Optimization.pdf:application/pdf},
}

@article{bubeck_convex_2015,
	title = {Convex {Optimization}: {Algorithms} and {Complexity}},
	shorttitle = {Convex {Optimization}},
	url = {http://arxiv.org/abs/1405.4980},
	doi = {10.48550/arXiv.1405.4980},
	abstract = {This monograph presents the main complexity theorems in convex optimization and their corresponding algorithms. Starting from the fundamental theory of black-box optimization, the material progresses towards recent advances in structural optimization and stochastic optimization. Our presentation of black-box optimization, strongly influenced by Nesterov's seminal book and Nemirovski's lecture notes, includes the analysis of cutting plane methods, as well as (accelerated) gradient descent schemes. We also pay special attention to non-Euclidean settings (relevant algorithms include Frank-Wolfe, mirror descent, and dual averaging) and discuss their relevance in machine learning. We provide a gentle introduction to structural optimization with FISTA (to optimize a sum of a smooth and a simple non-smooth term), saddle-point mirror prox (Nemirovski's alternative to Nesterov's smoothing), and a concise description of interior point methods. In stochastic optimization we discuss stochastic gradient descent, mini-batches, random coordinate descent, and sublinear algorithms. We also briefly touch upon convex relaxation of combinatorial problems and the use of randomness to round solutions, as well as random walks based methods.},
	urldate = {2023-10-12},
	author = {Bubeck, Sébastien},
	month = nov,
	year = {2015},
	note = {arXiv:1405.4980 [cs, math, stat]},
	keywords = {Mathematics - Optimization and Control, Statistics - Machine Learning, Computer Science - Computational Complexity, Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/7JTU9RKT/Bubeck - 2015 - Convex Optimization Algorithms and Complexity.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/8CADXSB9/1405.html:text/html},
}

@misc{fazel_ee_2014,
	address = {University of Washington, Paul Allen Center, Room CSE 230.},
	title = {{EE} 546 at {University} of {Wasthington} {Spring} 2014, {Nesterov} {Estimating} {Sequences} {Method}},
	url = {https://class.ece.uw.edu/546/2014spr/lectures/optimal.pdf},
	language = {en},
	urldate = {2023-10-14},
	author = {Fazel, Maryam},
	year = {2014},
	file = {optimal.pdf:/Users/hongdali/Zotero/storage/V2Q3HN2U/optimal.pdf:application/pdf},
}

@incollection{fornasier_introduction_2010,
	title = {An {Introduction} to {Total} {Variation} for {Image} {Analysis}},
	isbn = {978-3-11-022614-0},
	url = {https://www.degruyter.com/document/doi/10.1515/9783110226157.263/html},
	abstract = {These notes address various theoretical and practical topics related to Total Variationbased image reconstruction. It focuses ﬁrst on some theoretical results on functions which minimize the total variation, and in a second part, describes a few standard and less standard algorithms to minimize the total variation in a ﬁnite-differences setting, with a series of applications from simple denoising to stereo, or deconvolution issues, and even more exotic uses like the minimization of minimal partition problems.},
	language = {en},
	urldate = {2023-10-15},
	booktitle = {Theoretical {Foundations} and {Numerical} {Methods} for {Sparse} {Recovery}},
	publisher = {DE GRUYTER},
	author = {Chambolle, Antonin and Caselles, Vicent and Cremers, Daniel and Novaga, Matteo and Pock, Thomas},
	editor = {Fornasier, Massimo},
	month = jul,
	year = {2010},
	doi = {10.1515/9783110226157.263},
	keywords = {convex optimization, deconvolution, denoising, splitting algorithms, total variation, variational image reconstruction},
	pages = {263--340},
	file = {Chambolle et al. - 2010 - An Introduction to Total Variation for Image Analy.pdf:/Users/hongdali/Zotero/storage/36HV9FQJ/Chambolle et al. - 2010 - An Introduction to Total Variation for Image Analy.pdf:application/pdf},
}

@inproceedings{alamo_gradient_2019,
	title = {Gradient {Based} {Restart} {FISTA}},
	url = {https://ieeexplore.ieee.org/document/9029983},
	doi = {10.1109/CDC40024.2019.9029983},
	abstract = {Fast gradient methods (FGM) are very popular in the field of large scale convex optimization problems. Recently, it has been shown that restart strategies can guarantee global linear convergence for non-strongly convex optimization problems if a quadratic functional growth condition is satisfied [1], [2]. In this context, a novel restart FGM algorithm with global linear convergence is proposed in this paper. The main advantages of the algorithm with respect to other linearly convergent restart FGM algorithms are its simplicity and that it does not require prior knowledge of the optimal value of the objective function or of the quadratic functional growth parameter. We present some numerical simulations that illustrate the performance of the algorithm.},
	urldate = {2023-10-18},
	booktitle = {2019 {IEEE} 58th {Conference} on {Decision} and {Control} ({CDC})},
	author = {Alamo, Teodoro and Krupa, Pablo and Limon, Daniel},
	month = dec,
	year = {2019},
	note = {ISSN: 2576-2370},
	pages = {3936--3941},
	file = {IEEE Xplore Abstract Record:/Users/hongdali/Zotero/storage/W8B3RR7L/9029983.html:text/html;IEEE Xplore Full Text PDF:/Users/hongdali/Zotero/storage/79P9V3R6/Alamo et al. - 2019 - Gradient Based Restart FISTA.pdf:application/pdf},
}

@misc{alamo_restart_2019,
	title = {Restart {FISTA} with {Global} {Linear} {Convergence}},
	url = {http://arxiv.org/abs/1906.09126},
	doi = {10.48550/arXiv.1906.09126},
	abstract = {Fast Iterative Shrinking-Threshold Algorithm (FISTA) is a popular fast gradient descent method (FGM) in the field of large scale convex optimization problems. However, it can exhibit undesirable periodic oscillatory behaviour in some applications that slows its convergence. Restart schemes seek to improve the convergence of FGM algorithms by suppressing the oscillatory behaviour. Recently, a restart scheme for FGM has been proposed that provides linear convergence for non strongly convex optimization problems that satisfy a quadratic functional growth condition. However, the proposed algorithm requires prior knowledge of the optimal value of the objective function or of the quadratic functional growth parameter. In this paper we present a restart scheme for FISTA algorithm, with global linear convergence, for non strongly convex optimization problems that satisfy the quadratic growth condition without requiring the aforementioned values. We present some numerical simulations that suggest that the proposed approach outperforms other restart FISTA schemes.},
	urldate = {2023-10-18},
	publisher = {arXiv},
	author = {Alamo, Teodoro and Krupa, Pablo and Limon, Daniel},
	month = dec,
	year = {2019},
	note = {arXiv:1906.09126 [math]},
	keywords = {Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/B79AUCM7/Alamo et al. - 2019 - Restart FISTA with Global Linear Convergence.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/LZ7ISHFW/1906.html:text/html},
}

@article{calatroni_backtracking_2019,
	title = {Backtracking {Strategies} for {Accelerated} {Descent} {Methods} with {Smooth} {Composite} {Objectives}},
	volume = {29},
	issn = {1052-6234},
	url = {https://epubs.siam.org/doi/10.1137/17M1149390},
	doi = {10.1137/17M1149390},
	abstract = {Motivated by big data applications, first-order methods have been extremely popular in recent years. However, naive gradient methods generally converge slowly. Hence, much effort has been made to accelerate various first-order methods. This paper proposes two accelerated methods towards solving structured linearly constrained convex programming, for which we assume composite convex objective that is the sum of a differentiable function and a possibly nondifferentiable one. The first method is the accelerated linearized augmented Lagrangian method (LALM). At each update to the primal variable, it allows linearization to the differentiable function and also the augmented term, and thus it enables easy subproblems. Assuming merely convexity, we show that LALM owns \$O(1/t)\$ convergence if parameters are kept fixed during all the iterations and can be accelerated to  \$O(1/t{\textasciicircum}2)\$ if the parameters are adapted, where \$t\$ is the number of total iterations. The second method is the accelerated linearized alternating direction method of multipliers (LADMM). In addition to the composite convexity, it further assumes two-block structure on the objective. Different from classic alternating direction method of multipliers, our method allows linearization to the objective and also augmented term to make the update simple. Assuming strong convexity on one block variable, we show that LADMM also enjoys \$O(1/t{\textasciicircum}2)\$ convergence with adaptive parameters.  This result is a significant improvement over that in [Goldstein et. al, SIAM J. Imag. Sci., 7 (2014), pp. 1588--1623], which requires strong convexity on both block variables and no linearization to the objective or augmented term. Numerical experiments are performed on quadratic programming, image denoising, and support vector machine. The proposed accelerated methods are compared to nonaccelerated ones and also existing accelerated methods. The results demonstrate the validity of acceleration and superior performance of the proposed methods over existing ones.},
	number = {3},
	urldate = {2023-10-19},
	journal = {SIAM Journal on Optimization},
	author = {Calatroni, Luca and Chambolle, Antonin},
	month = jan,
	year = {2019},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {1772--1798},
	file = {Submitted Version:/Users/hongdali/Zotero/storage/PA25ST8M/Calatroni and Chambolle - 2019 - Backtracking Strategies for Accelerated Descent Me.pdf:application/pdf},
}

@article{aujol_convergence_2023,
	title = {Convergence rates of the {Heavy}-{Ball} method under the Łojasiewicz property},
	volume = {198},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-022-01770-2},
	doi = {10.1007/s10107-022-01770-2},
	abstract = {In this paper, a joint study of the behavior of solutions of the Heavy Ball ODE and Heavy Ball type algorithms is given. Since the pioneering work of Polyak (USSR Comput Math Math Phys 4(5):1–17, 1964), it is well known that such a scheme is very efficient for \$\$C{\textasciicircum}2\$\$strongly convex functions with Lipschitz gradient. But much less is known when only growth conditions are considered. Depending on the geometry of the function to minimize, convergence rates for convex functions, with some additional regularity such as quasi-strong convexity, or strong convexity, were recently obtained in Aujol et al. (Convergence rates of the Heavy-Ball method for quasi-strongly convex optimization, 2020). Convergence results with much weaker assumptions are given in the present paper: namely, linear convergence rates when assuming a growth condition (which amounts to a Łojasiewicz property in the convex case). This analysis is firstly performed in continuous time for the ODE, and then transposed for discrete optimization schemes. In particular, a variant of the Heavy Ball algorithm is proposed, which converges geometrically whatever the parameters choice, and which has the best state of the art convergence rate for first order methods to minimize composite non smooth convex functions satisfying a Łojasiewicz property.},
	language = {en},
	number = {1},
	urldate = {2023-10-19},
	journal = {Mathematical Programming},
	author = {Aujol, J.-F. and Dossal, Ch. and Rondepierre, A.},
	month = mar,
	year = {2023},
	keywords = {90C25, ODEs, 90C30, 65K10, Heavy Ball method, Łojasiewicz property, Lyapunov function, Optimization, Rate of convergence},
	pages = {195--254},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/URSACZIH/Aujol et al. - 2023 - Convergence rates of the Heavy-Ball method under t.pdf:application/pdf},
}

@article{beck_fast_2009,
	title = {Fast {Gradient}-{Based} {Algorithms} for {Constrained} {Total} {Variation} {Image} {Denoising} and {Deblurring} {Problems}},
	volume = {18},
	issn = {1941-0042},
	url = {https://ieeexplore.ieee.org/document/5173518},
	doi = {10.1109/TIP.2009.2028250},
	abstract = {This paper studies gradient-based schemes for image denoising and deblurring problems based on the discretized total variation (TV) minimization model with constraints. We derive a fast algorithm for the constrained TV-based image deburring problem. To achieve this task, we combine an acceleration of the well known dual approach to the denoising problem with a novel monotone version of a fast iterative shrinkage/thresholding algorithm (FISTA) we have recently introduced. The resulting gradient-based algorithm shares a remarkable simplicity together with a proven global rate of convergence which is significantly better than currently known gradient projections-based methods. Our results are applicable to both the anisotropic and isotropic discretized TV functionals. Initial numerical results demonstrate the viability and efficiency of the proposed algorithms on image deblurring problems with box constraints.},
	number = {11},
	urldate = {2023-10-19},
	journal = {IEEE Transactions on Image Processing},
	author = {Beck, Amir and Teboulle, Marc},
	month = nov,
	year = {2009},
	note = {Conference Name: IEEE Transactions on Image Processing},
	pages = {2419--2434},
	file = {IEEE Xplore Abstract Record:/Users/hongdali/Zotero/storage/G4ZXMUZG/5173518.html:text/html;IEEE Xplore Full Text PDF:/Users/hongdali/Zotero/storage/J2T9LVVK/Beck and Teboulle - 2009 - Fast Gradient-Based Algorithms for Constrained Tot.pdf:application/pdf},
}

@article{chambolle_introduction_2016,
	series = {Acta {Numerica}},
	title = {An introduction to continuous optimization for imaging},
	volume = {25},
	url = {https://hal.science/hal-01346507},
	doi = {10.1017/S096249291600009X},
	abstract = {A large number of imaging problems reduce to the optimization of a cost function , with typical structural properties. The aim of this paper is to describe the state of the art in continuous optimization methods for such problems, and present the most successful approaches and their interconnections. We place particular emphasis on optimal first-order schemes that can deal with typical non-smooth and large-scale objective functions used in imaging problems. We illustrate and compare the different algorithms using classical non-smooth problems in imaging, such as denoising and deblurring. Moreover, we present applications of the algorithms to more advanced problems, such as magnetic resonance imaging, multilabel image segmentation, optical flow estimation, stereo matching, and classification.},
	urldate = {2023-10-19},
	journal = {Acta Numerica},
	author = {Chambolle, Antonin and Pock, Thomas},
	year = {2016},
	note = {Publisher: Cambridge University Press (CUP)},
	keywords = {algorithms, convex analysis, imaging, nonsmooth optimization},
	pages = {161--319},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/WYGGWYVC/Chambolle and Pock - 2016 - An introduction to continuous optimization for ima.pdf:application/pdf},
}

@book{beck_first-order_nodate,
	series = {{MOS}-{SIAM} {Series} in {Optimization}},
	title = {First-{Order} {Methods} in {Optimization} {\textbar} {SIAM} {Publications} {Library}},
	isbn = {978-1-61197-498-0},
	url = {https://epubs.siam.org/doi/book/10.1137/1.9781611974997},
	language = {en},
	urldate = {2023-10-19},
	publisher = {SIAM},
	author = {Beck, Amir},
	file = {First-Order Methods in Optimization  SIAM Publication.pdf:/Users/hongdali/Zotero/storage/P2HFAVVQ/First-Order Methods in Optimization  SIAM Publication.pdf:application/pdf;Snapshot:/Users/hongdali/Zotero/storage/88BHKZ6Y/1.html:text/html},
}

@misc{ahn_understanding_2022,
	title = {Understanding {Nesterov}'s {Acceleration} via {Proximal} {Point} {Method}},
	url = {http://arxiv.org/abs/2005.08304},
	doi = {10.48550/arXiv.2005.08304},
	abstract = {The proximal point method (PPM) is a fundamental method in optimization that is often used as a building block for designing optimization algorithms. In this work, we use the PPM method to provide conceptually simple derivations along with convergence analyses of different versions of Nesterov's accelerated gradient method (AGM). The key observation is that AGM is a simple approximation of PPM, which results in an elementary derivation of the update equations and stepsizes of AGM. This view also leads to a transparent and conceptually simple analysis of AGM's convergence by using the analysis of PPM. The derivations also naturally extend to the strongly convex case. Ultimately, the results presented in this paper are of both didactic and conceptual value; they unify and explain existing variants of AGM while motivating other accelerated methods for practically relevant settings.},
	urldate = {2023-11-04},
	publisher = {arXiv},
	author = {Ahn, Kwangjun and Sra, Suvrit},
	month = jun,
	year = {2022},
	note = {arXiv:2005.08304 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/PZBWUWW5/Ahn and Sra - 2022 - Understanding Nesterov's Acceleration via Proximal.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/WSSGK9Q4/2005.html:text/html},
}

@article{fercoq_adaptive_2019,
	title = {Adaptive restart of accelerated gradient methods under local quadratic growth condition},
	volume = {39},
	issn = {0272-4979, 1464-3642},
	url = {http://arxiv.org/abs/1709.02300},
	doi = {10.1093/imanum/drz007},
	abstract = {By analyzing accelerated proximal gradient methods under a local quadratic growth condition, we show that restarting these algorithms at any frequency gives a globally linearly convergent algorithm. This result was previously known only for long enough frequencies. Then, as the rate of convergence depends on the match between the frequency and the quadratic error bound, we design a scheme to automatically adapt the frequency of restart from the observed decrease of the norm of the gradient mapping. Our algorithm has a better theoretical bound than previously proposed methods for the adaptation to the quadratic error bound of the objective. We illustrate the efficiency of the algorithm on a Lasso problem and on a regularized logistic regression problem.},
	number = {4},
	urldate = {2023-11-08},
	journal = {IMA Journal of Numerical Analysis},
	author = {Fercoq, Olivier and Qu, Zheng},
	month = oct,
	year = {2019},
	note = {arXiv:1709.02300 [math]},
	keywords = {Mathematics - Optimization and Control},
	pages = {2069--2095},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/83L7V5YY/Fercoq and Qu - 2019 - Adaptive restart of accelerated gradient methods u.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/26N2YTES/1709.html:text/html},
}

@misc{goldstein_field_2016,
	title = {A {Field} {Guide} to {Forward}-{Backward} {Splitting} with a {FASTA} {Implementation}},
	url = {http://arxiv.org/abs/1411.3406},
	abstract = {Non-differentiable and constrained optimization play a key role in machine learning, signal and image processing, communications, and beyond. For high-dimensional minimization problems involving large datasets or many unknowns, the forward-backward splitting method provides a simple, practical solver. Despite its apparently simplicity, the performance of the forward-backward splitting is highly sensitive to implementation details. This article is an introductory review of forward-backward splitting with a special emphasis on practical implementation concerns. Issues like stepsize selection, acceleration, stopping conditions, and initialization are considered. Numerical experiments are used to compare the effectiveness of different approaches. Many variations of forward-backward splitting are implemented in the solver FASTA (short for Fast Adaptive Shrinkage/Thresholding Algorithm). FASTA provides a simple interface for applying forward-backward splitting to a broad range of problems.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Goldstein, Tom and Studer, Christoph and Baraniuk, Richard},
	month = dec,
	year = {2016},
	note = {arXiv:1411.3406 [cs]},
	keywords = {G.1.6, Mathematics - Numerical Analysis},
	file = {arXiv.org Snapshot:/Users/hongdali/Zotero/storage/PZZLNWLU/1411.html:text/html;Full Text PDF:/Users/hongdali/Zotero/storage/LZW8GXGH/Goldstein et al. - 2016 - A Field Guide to Forward-Backward Splitting with a.pdf:application/pdf;Goldstein et al. - 2016 - A Field Guide to Forward-Backward Splitting with a.pdf:/Users/hongdali/Zotero/storage/HPWHH37K/Goldstein et al. - 2016 - A Field Guide to Forward-Backward Splitting with a.pdf:application/pdf},
}

@article{beck_fast_2009-1,
	title = {A {Fast} {Iterative} {Shrinkage}-{Thresholding} {Algorithm} for {Linear} {Inverse} {Problems}},
	volume = {2},
	issn = {1936-4954},
	url = {http://epubs.siam.org/doi/10.1137/080716542},
	doi = {10.1137/080716542},
	abstract = {We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be signiﬁcantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude.},
	language = {en},
	number = {1},
	urldate = {2023-11-16},
	journal = {SIAM Journal on Imaging Sciences},
	author = {Beck, Amir and Teboulle, Marc},
	month = jan,
	year = {2009},
	pages = {183--202},
	file = {Beck and Teboulle - 2009 - A Fast Iterative Shrinkage-Thresholding Algorithm .pdf:/Users/hongdali/Zotero/storage/H7CGKLL3/Beck and Teboulle - 2009 - A Fast Iterative Shrinkage-Thresholding Algorithm .pdf:application/pdf},
}
