\documentclass[]{article}
\usepackage{amsmath}
\usepackage{amsfonts} 
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{anyfontsize} % fix font size warning. 
\usepackage{url} 
\urlstyle{same} % fix wacky url links in bib entries. 
\usepackage{lineno}
% MANUAL SCRIPT LINE NUMBER!!!!
\renewcommand{\linenumberfont}{\normalfont\bfseries\tiny\color{gray}}
\linenumbers
% \usepackage{minted}

% Basic Type Settings ----------------------------------------------------------
\usepackage[margin=1in,footskip=0.25in]{geometry}
\linespread{1}  % double spaced or single spaced
\usepackage[fontsize=11pt]{fontsize}
\usepackage{authblk}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[subsection]       % Theorem counter global 
\newtheorem{prop}{Proposition}[section]  % proposition counter is section
\newtheorem{lemma}{Lemma}[subsection]  % lemma counter is subsection
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}[subsection]
{
    % \theoremstyle{plain}
    \newtheorem{assumption}{Assumption}
}
\newtheorem{exmp}{Example}[section]
\numberwithin{equation}{subsection}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}
\usepackage[final]{graphicx}
\usepackage{listings}
\usepackage{courier}
\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\newcommand{\indep}{\perp \!\!\! \perp}
\usepackage{wrapfig}
\graphicspath{{.}}
\usepackage{fancyvrb}

%%
%% Julia definition (c) 2014 Jubobs
%%
\usepackage[T1]{fontenc}
\usepackage{beramono}
\usepackage[usenames,dvipsnames]{xcolor}
\lstdefinelanguage{Julia}%
  {morekeywords={abstract,break,case,catch,const,continue,do, else, elseif,%
      end, export, false, for, function, immutable, import, importall, if, in,%
      macro, module, otherwise, quote, return, switch, true, try, type, typealias,%
      using, while},%
   sensitive=true,%
   alsoother={$},%
   morecomment=[l]\#,%
   morecomment=[n]{\#=}{=\#},%
   morestring=[s]{"}{"},%
   morestring=[m]{'}{'},%
}[keywords,comments,strings]%
\lstset{%
    language         = Julia,
    basicstyle       = \ttfamily,
    keywordstyle     = \bfseries\color{blue},
    stringstyle      = \color{magenta},
    commentstyle     = \color{ForestGreen},
    showstringspaces = false,
}

\title{MATH 590 2023 FALL REPORT}
\author{HONGDA LI}

\begin{document}
\maketitle

\begin{abstract}
    In this paper, we review the paper written by Walkington \cite{noel_nesterovs_nodate} on the topic of proximal gradient with Nesterov accelerations. 
    We compare the performance of the FISTA method and some of its variants with numerical experiments on the total variation minimization problem; in addition, we propose a heuristic estimation of the strong convexity parameter and demonstrate that it converges faster when applied. 
    We give a literature review on the frontier for both the theories and applications around FISTA. 
    We correct one misconception that occurred in Walkington \cite{noel_nesterovs_nodate} regarding Nesterov's proof of lower bound on the optimality of first-order algorithms.  
    We present a better proof of linear convergence of FISTA under strong convexity assumption from Beck \cite[theorem 10.7.7]{beck_first-order_nodate} by eliminating an identity used in their proof. 
    Finally, we use the Forward Backward Envelope to adapt smooth non-smooth additive objective to Nesterov's generic accelerated gradient algorithm \cite[2.2.7]{nesterov_lecture_2018}. 
\end{abstract}


\section{Preliminaries}\label{sec:preliminaries}
    In this section, We initiate the discussion by denoising a one-dimensional signal. 
    The dual objective function of the problem derived in this section motivates the use of Accelerated Proximal Gradient with smooth, non-smooth composite objective function.
    We summarized it from Walkington \cite{noel_nesterovs_nodate}, sections 1 and 4. 
    \subsection{Signal Denoising in One Dimension}
        Let a one dimensional singal be $u: \mathbb [0, 1]\mapsto \mathbb R$ and $u$. 
        Regularizing the derivative of the signal using the L1 norm helps recover the digital signal if it's a piecewise constant function. 
        This is called a Total Variation (TV) minimization. 
        Let $\hat u$ denote an observation of $u$ corrupted by noise. 
        The denoised signal is the minimizer of f(u),
        \[
            f(u) = \int_0^1 \frac{1}{2} 
            (u - \hat u)^2 + \alpha |u'|dt. 
        \]
        Practical implementations for modern computing devices would necessitate discretization of the integral. 
        \par
        We use the trapezoidal rule and second-order forward difference for the derivative. 
        Let $\hat u \in \mathbb R^{N+1}$, a vector in the form of $\hat u = [\hat u_0\; \cdots \; \hat u_{N}]$, let $t_0<  \cdots <t_N$ be a sequence of time corresponded to each observation of $\hat u_i$. 
        The time intervals are $h_i = t_{i} - t_{i-1}$ for $i=1, \cdots, N$, not necessarily equally spaced, making this formulation below is slightly more general than Walkington\cite{noel_nesterovs_nodate}. 
        We derive the approximation of the integral. Denote $s_i = u_i - \hat u_i$. 
        \begin{align*}
            \frac{1}{2}\int_{0}^{1} (u - \hat u)^2 dt + 
            \alpha \int_0^1 |u'| dt
            &\approx
            \frac{1}{2}
            \sum_{i = 0}^{N}
            \left(
                \frac{s_i^2 + s_{i + 1}^2}{2}
            \right)h_{i + 1}
            + 
            \alpha
            \sum_{i = 1}^{N}
            \left|
                \frac{u_{i} - u_{i - 1}}{h_{i + 1}}
            \right|
            \\
            & \triangleright\; \text{let } 
            C\in \mathbb R^{N\times (N + 1)} \text{ be upper bi-diagonal with }(1, -1)
            \\
            &= \frac{1}{2}\left(
                \frac{s_0^2h_1}{2} + \frac{s_N^2h_N}{2}
                + 
                \sum_{i = 1}^{N - 1}s_i^2 h_i
            \right) + \alpha\Vert Cu\Vert_1
            \\
            & 
            \triangleright \; \text{using } D\in \mathbb R^{N \times (N + 1)},
            \\
            & \triangleright\; D := \text{diag}(h_1/2, h_1, h_2, \cdots, h_N, h_N/2)
            \\
            &= 
            \frac{1}{2}\langle u - \hat u, D(u - \hat u)\rangle + \alpha \Vert Cu\Vert_1. 
        \end{align*}
    The above formulation suggests a smooth, non-smooth additive composite objective for $f(u)$. 
    The Proximal Gradient method and its variants can solve this optimization problem. 
    Unfortunately, the non-smooth part $\alpha\Vert Cu\Vert_1$ presents computational difficulty if matrix $C$ is unfriendly for the prox operator. 
    One way to bypass the difficulty involves reformulating with $p = Cu$ and solving the dual problem. 
    \subsection*{Dual Reformulation}
        Let $p = Cu$, $C\in \mathbb R^{(N + 1)\times N}$ with $D \in \mathbb R^{(N + 1)\times (N + 1)}$, we reformulate it into 
        \[
            \min_{u\in \mathbb R^{N + 1}}     
            \left\lbrace
                \left.
                    \underbrace{\frac{1}{2}\langle (u - \hat u), D(u - \hat u)}_{f(u)}\rangle 
                    + 
                    \underbrace{\alpha \Vert p\Vert_1}_{h(p)}
                \right| 
                p = Cu
            \right\rbrace, 
        \]
        producing Lagrangian of the form 
        \[
            \mathcal L((u, p), \lambda) = 
            f(u) + h(p) + \langle \lambda, p - Cu\rangle. 
        \]
        The dual is
        \begin{align*}
            - g(\lambda) &:= \inf_{(u, p)\in \mathbb R^{N + 1}\times \mathbb R^N}
            \left\lbrace
                \mathcal L({(u, p), \lambda})
            \right\rbrace
            \\
            &= \inf_{(u, p)\in \mathbb R^{N + 1}\times \mathbb R^N}
            \left\lbrace
                f(u) + h(p) + \langle \lambda, p - Cu\rangle
            \right\rbrace
            \\
            &= 
            \inf_{u\in \mathbb R^{N + 1}}
            \left\lbrace
                f(u) - \langle \lambda, Cu\rangle 
                + 
                \inf_{p\in \mathbb R^{N}}
                \left\lbrace
                    h(p) + \langle \lambda, p\rangle  
                \right\rbrace
            \right\rbrace
            \\
            &\le
            -f^\star (-C^T\lambda) - h^\star(p). 
        \end{align*}
        The theorem of strong duality applies hence equality. 
        With the assumption that $D$ is positive definite, we have 
        \[
            - g(\lambda) = -\frac{1}{2}    \Vert C^T\lambda\Vert^2_{D^{-1}} - 
            \langle \hat u, C^T \lambda\rangle - 
            \delta_{[-\alpha, \alpha]^N}(p). 
        \]
        Observe that the above admits a hyperbox indicator function that makes the prox operator friendlier because the proximal operator of the indicator is projection; in the case of projecting onto the box, the operator is simple. 
        Given dual variable $\lambda$, primal is obtained by 
        \begin{align*}
            u &= \text{argmin}_u \mathcal L((u, p), \lambda) 
            \\
            \partial_u \mathcal L((u, p), \lambda) &= D(u - \hat u) - C^T\lambda = \mathbf 0
            \\
            \implies u &= \hat u + D^{-1}C^T\lambda. 
        \end{align*}
        $-g(\lambda)$ is easier to optimize, and obtaining the primal solution is also simple since $D^{-1}$ is a diagonal matrix. 
    \subsection{FISTA has Worse Convergence Guarantee for Strongly Convex Objectives}
        The dual objective for a total variation minimization problem is a strongly convex and Lipschitz smooth function because of the norm induced by the positive definite matrix $D^{-1}$. 
        It's in a form where FISTA proposed by \cite{beck_fast_2009-1} can solve with a convergence rate of $\mathcal O(1/k^2)$ on the objective value of the function. 
        However, highlighted in Walkington\cite{noel_nesterovs_nodate}, the proximal gradient method without acceleration achieves $\mathcal O\left((1 - 1/\kappa)^k\right)$ convergence rate. 
        Which is faster.
        The parameter $\kappa$ is the condition number; in this case, it would be $L/\alpha$, where $L$ is the Lipschitz smooth constant of $g(u)$ and $\alpha$ is the strong convexity constant for $g(u)$. 
        \par
        We emphasize that the Proximal Gradient without acceleration has better theoretical convergence results than the accelerated version for the class of strongly convex objectives. 
        It sparks the discussion in this paper on the variants of FISTA, hoping to provide some insights on why Nesterov's momentum-based method's inability to adapt the convergence rate with objective has strong convexity. 
        For the terminologies, we use FISTA to refer to the proximal gradient method presented by Beck and Teboulle\cite{beck_fast_2009-1}. We use the Accelerated Proximal Gradient method (APG) to refer to the first-order acceleration algorithms developed/inspired by FISTA. 
        \par
        Finally, whether the original FISTA\cite{beck_fast_2009} or Nesterov Accelerated gradient from 1983 has linear convergence with the presence of strong convexity (or potentially other weaker conditions) is not known during our research and literature review. 

    \subsection{Outline of the Paper}
        \hyperref[sec:Literatures]{Section \ref*{sec:Literatures}} consists of 3 parts. 
        The first part reviews the literature on the problem of Total Variation (TV) minimization for image/signal denoising and deblurring. 
        Presenting FISTA and its variants is the second part. 
        The third part reviews the algorithmic tricks and improvements applied to the APG. 
        \hyperref[sec:optimal_lower_bound]{Section \ref*{sec:optimal_lower_bound}} addresses a mistake made in Walkington's writing \cite[theorem 2.4]{noel_nesterovs_nodate}. 
        We will discuss a first-order method and how a different function achieves the lower complexity bound on the objective value and iterates for a fixed iteration. 
        We discuss how omitting the details of this theorem creates potential misconceptions of other frontier research ideas. 
        \hyperref[sec:fista_strong_convexity]{Section \ref*{sec:fista_strong_convexity}} presents a proof that I adapted from Amir Beck's writing \cite[theorem 10.7.7]{beck_first-order_nodate}. 
        The proof is slightly more general, removing one equality to strengthen interpretability and generality. 
        \hyperref[sec:numerics]{Section \ref*{sec:numerics}} presents plots of convergence and results of applying variants of APG to the TV problem. 
        

\section{Literatures Review}\label{sec:Literatures}
    \subsection{Total Variation Minimizations}
        Rudin-Osher and Fatemi introduced the Total Variation (TV) minimization method in \cite{rudin_nonlinear_1992}. 
        They pioneer the theories of TV minimization by solving PDE. 
        They discussed the empirical observation that the L1 regularization term produces sharper images. 
        Walkington \cite{noel_nesterovs_nodate} gives a basic formulation of one-dimensional signal denoising. 
        However, it's essential to keep in mind that this is a problem that motivates a variety of modern computational methods and theories. 
        We will list some of them for context. 
        \par
        Goldstein et al. in \cite[3.2.1]{goldstein_field_2016} showcased the dual reformulation of a 2D signal recovery with $\Vert \nabla u\Vert$ as the regularizations term. 
        We note that this norm is without the squared. 
        A more hardcore, detailed coverage of reformulating the dual with L1 penalty terms for 2D signal recovery is in \cite{beck_fast_2009}. 
        For a complete survey of the state of arts computational methods applied to TV minimizations, see Chambolle \cite{chambolle_introduction_2016}. 
        For a detailed exposition of mathematical theories regarding variational analysis on different types of TV problems and statistical inferences-based interpretations of the TV regularization term, consult the work by Chambolle et al.\cite{fornasier_introduction_2010}.
        For frontier work of applying non-convex penalty term and its theoretical guarantee consult \cite{an_enhanced_2023}, \cite{an_springback_2022}. 
    \subsection*{Variants of FISTA}
        % Walkington's writing on the method of V-FISTA and accelerated gradient\cite[section 4, section 3]{noel_nesterovs_nodate} consists of proofs that are too short and uninformative for good understanding. 
        % The frustration motivates us to look for better proofs of the algorithm's convergence rate in other literature. 
        % It was a surprise that Walkington did not cite Nesterov's new book in 2018\cite{nesterov_lectures_2018}. 
        % We contextualize Walkington's approach with Amir Beck's book\cite{beck_first-order_nodate} and Nesterov's book \cite{nesterov_lectures_2018}. 
        % \par
        Different variants of FISTA differ by the sequence involved for their momentum method. 
        Choosing different parameters in \hyperref[alg:generic_FISTA]{algorithm \ref*{alg:generic_FISTA}} produces variants of FISTA. 
        \begin{algorithm}[H]
            \begin{algorithmic}[1]
                \STATE{\textbf{Input: }($g, h, x^{(0)}$)}
                \STATE{$y^{(0)} = x^{(0)}$, $\kappa = L/\sigma$} 
                \FOR{$k = 0, 1, \cdots$}
                    \STATE{$x^{(k + 1)} = T_L y^{(k)}$}
                    \STATE{$y^{(k + 1)} = x^{(k + 1)} + \theta_{k + 1}(x^{(k + 1)} - x^{(k)})$}
                    \STATE{Execute subroutine $\mathcal S$. }
                \ENDFOR
            \end{algorithmic}
            \caption{Generic FISTA}
            \label{alg:generic_FISTA}
        \end{algorithm}
        The scope of \hyperref[alg:generic_FISTA]{algorithm \ref*{alg:generic_FISTA}} considers the additive composition of convex smooth and nonsmooth $F = g + h$ function with $g$ being a $L$-smooth function. 
        Changing $T_L, \theta_{k + 1}$ and $\mathcal S$, produce different variants of FISTA. 
        \begin{itemize}
            \item [1.] 
                Original FISTA proposed by Beck \cite{beck_fast_2009-1} considers $\theta_{k + 1} = (t_k - 1)/t_{k + 1}$, $t_{k + 1}(t_{k + 1} - 1) = t_{k}^2$, with $T_L x = \text{prox}_{L^{-1}h}(x  - L^{-1}\nabla g(x))$ and $t_0 = 1$. 
            It achieves $\mathcal O(1/k^2)$ on the objective value. 
            Our literature review didn't discover proofs for the convergence of the iterates. We also didn't find proofs for a convergence rate faster than $\mathcal O((1 - 1/\kappa)^k)$ under strong convexity. 
            \item [2.] 
                This is a variant where $\theta_{k + 1} = (n + a - 1)/a$, for $a > 2$. $\mathcal T_L$ is the same as (1.). 
            Proved in Chambolle, Dossal \cite{chambolle_convergence_2015}, its iterates of this version of FISTA exhibit weak convergence. $T_L$ is the same as (1.). 
            \item [3.] 
                Using $\theta_{k + 1} = (\sqrt{\kappa} - 1)/(\sqrt{\kappa} + 1)$ where $\kappa = L/\sigma$, with $\sigma$ being the strong convexity constant produces V-FISTA in Beck \cite[10.7.7]{beck_first-order_nodate}\cite[3.3]{noel_nesterovs_nodate}. $T_L$ is the same as (1.). 
            \item [4.] 
                A modification we proposed is based on (3.), but it estimates $\sigma$, the strong convexity constant based $x^{(k)}, x^{(k + 1)}, \nabla g(x^{(k + 1)}), g(x^{(k)})$ using 
                \[
                    \sigma \approx \langle \nabla g(y^{(k + 1)}) - \nabla g(y^{(k)}), y^{(k + 1)} - y^{(k)}\rangle/ 
                    \Vert y^{(k + 1)} - y^{(k)}\Vert^2. 
                \]
            It yields excellent results for the numerical experiments. 
            \item [5.] 
                MFISTA in Beck\cite{beck_fast_2009} is produced by adding $\mathcal S$ to be the procedure
                \[
                    (y^{(k + 1)}, t_{k + 1}) = \begin{cases}
                        (x^{(k + 1)}, 1) & F(y^{(k + 1)}) > F(x^{(k + 1)}),
                        \\
                        (y^{(k + 1)}, t_{k + 1}) & \text{else}. 
                    \end{cases}
                \]
                This condition asserts a monotone decrease in the objective value. If the objective with momentum increases, it resets the momentum for the next iteration.  It has a convergence rate of $\mathcal O(1/k^2)$ back then. Our review of the literature didn't confirm the existence of a proof where it has a faster convergence than $\mathcal O((1 - 1/\kappa)^k)$ under the presence of strong convexity. 
        \end{itemize}
        For our problem posed in section \ref*{sec:preliminaries}, the V-FISTA algorithm variant (3.), when applied to the dual objective, achieves a convergence rate of $\mathcal O((1 - 1/\sqrt{\kappa})^k)$ for the function objective. 
        For larger $\kappa$, this produces a significantly better convergence rate than gradient descent, which is $\mathcal O((1 - 1/\kappa)^k)$, and FISTA, which is $\mathcal O(1/k^2)$. 
        \par
        Variants (3.) is simple; unfortunately, obtaining $\sigma$ in itself could be prohibitively expensive and require knowledge about the inverse of hessian (in the case of our TV Minimization problem). 
        Underestimation of $\sigma$ slows down the convergence rate. 
        This observation sparks research interest in a method that achieves approximately $\mathcal O((1 - 1/\sqrt{k})^k)$ linear convergence rate for strongly convex objectives and still retains $\mathcal O(1/k^2)$ for Lipschitz-smooth functions in general. 
        One of the other interests is designing a unified theoretical framework to describe all variants of APG. 
        \par
        To address the first issue, people use the idea of restarting FISTA.
        Earlier attempts proved an asymptotic fast linear convergence rate under quadratic growth conditions by triggering the restart of FISTA based on the gradient mapping norm. 
        See \cite{alamo_gradient_2019}\cite{fercoq_adaptive_2019}. 
        Aujol et al.\cite{aujol_fista_2022} proposed an automatic restart algorithm that achieves fast linear convergence without knowing the prior strong convexity (or weaker quadratic growth condition) parameter $\sigma$. 
        Their bound is not asymptotic. 
        Later, they developed the idea into a parameter-free algorithm in their work \cite{aujol_parameter-free_2023}. 
        The interests gather around restarting FISTA because spending too much computational effort would be competing against the Proximal Quasi-Newton method, questioning the use of momentum in the first place. 
        \par
        On the theoretical side, Su et al. \cite{su_differential_2015} identifies a second-order differential equation with the exact limit of (1.). 
        A dynamical system understanding of FISTA and APG, in general, enables a wider variety of mathematical tools. 
        For example, in Attouch and Peypouquet \cite{attouch_rate_2016}, they showed a $o(1/k^2)$ convergence rate of variant (2.) based on the ODE understanding. 
        We emphasize that it's the little-o and not the big-O. 
        In Nesterov\cite{nesterov_lecture_2018}, he proposed a generic algorithm that can derive variants (1.), (3.), and more using the idea of Estimating Sequences and Functions. 
        He constructed a proof of convergence on his generic algorithm, demonstrating both linear and sub-linear convergence rates (depending on the parameter) on the functions' objective value without assuming the minimizers' existence. 
        For Nesterov's involvement in proving convergence of APG in the non-convex settings, consult \cite{necoara_linear_2019}. 
        For a theoretical underpinning of Nesterov's generic method in his book, consult Ahn and Sra\cite{ahn_understanding_2022}. 
        They derived a lot of variants of APGs using the Proximal Point method of Rockafellar and discussed the unified theme of a "similar triangle" behind the Nesterov APG method. 
        \par
        Finally, the most recent hardcore idea in theory and practice is from Jang et al. \cite{jang_computer-assisted_2023}. 
        They squeezed out a constant from the convergence rate of FISTA by formulating the search for a faster algorithm as a QCQP. 
        They call their algorithm OptISTA. 
        Their approach is based on the performance estimation problem (PEP). 


\section{Nesterov's Lower Bound Clarified}\label{sec:optimal_lower_bound}
    Nesterov discussed his claim of the lower convergence rate for the first-order method on differentiable function in his book\cite{nesterov_lectures_2018}. 
    Walkington\cite{noel_nesterovs_nodate} rephrased his work with one crucial mistake in understanding Nesterov's claim. 
    \par
    A precise understanding is required to prevent confusion and lack of forethought in further research.
    We detail Nesterov's claim and provide context for understanding the mistakes in Walkington. 
    We comment on how the claim relates to other works at the end of the section. 
    \newcommand*{\GAfirst}{\text{GA}^{\text{1st}}} % DEFINE LOCAL COMMANDS
    We use the following notations
    \begin{itemize}
        \item [1.] Let $\mathcal A^k_fx^{(0)}$ denotes the solution of the k-th iterate $x^{(k)}$ generated by an algorithm $\mathcal A \in \GAfirst$, with initial guess $x^{(0)}$, objective function $f$. 
        With this notation, $x^{(1)} = \mathcal A_f x^{(0)}$ and $(\mathcal A_f^{k}x^{(0)})_{k \in \mathbb N}$ denotes the sequence generated by $\mathcal A \in \GAfirst$, with $f$.
        \item [2.] Let $\mathcal F^{1, 1}_L$ denote the set of convex functions with $L$-Lipschitz gradient mapping from $\mathbb R^n$ to $\mathbb R$. 
    \end{itemize}
    \subsection{First-order Method}
        The following is rephrased from Assumption \cite[2.1.4]{nesterov_lecture_2018}. 
        We came up with the two examples to illustrate the definition for better understanding. 
        \begin{definition}[First Order Method]
            We are in $\mathbb R^n$ for now. Given $x^{(0)} \in \mathbb R^n$, an iterative algorithm generates sequence of $\left(x^{(n)}\right)_{n \in \mathbb N}$ in the space. All $\mathcal A \in \GAfirst$ satisfy that 
            \begin{align*}
                 x^{(j + 1)}:= \mathcal A_f^{j + 1}x^{(0)} \in \left\{x^{(0)}\right\} + 
                \text{span}\left\{\nabla f\left(x^{(i)}\right)\right\}_{i = 1}^{j - 1} \quad \forall f, \forall 1\le j \le k -1. 
            \end{align*}
        \end{definition}
        \begin{exmp}[Fixed Step Descent]
            The method of fixed-step gradient descent, $x^{(k + 1)} = x^{(k)} - L^{-1}\nabla f(x^{(k)})$ is $ \bar {\mathcal A}\in \GAfirst$ achieves a maximal decrease in objective value for all $f\in \mathcal F_{L}^{1, 1}$ given $x^{(k)}$, it can be understood as 
            \[
                \bar {\mathcal A} \in 
                \underset{_{\mathcal A\in \GAfirst}}{\text{argmin}}
                \max_{f\in \mathcal F_L^{1, 1}} \left\lbrace
                    f\left(
                        \mathcal A_f x^{(k)}
                    \right)
                \right\rbrace. 
            \]
            This method is memoryless because it only matters what $x^{(k)}$, prior iterate $x^{(i)}, 1\le i \le k-1$ plays no role. 
        \end{exmp}
        \begin{exmp}[Steepest Descent]
            Fix some $f, x^{(k)}$, the method of steepest descent would be $\bar {\mathcal A}\in \GAfirst$ and it's 
            \[
                \bar {\mathcal A}\in  \underset{\mathcal A \in \GAfirst}{\text{argmin}}
                \left\lbrace
                    f\left(
                        \mathcal A_f x^{(k)}
                    \right)
                \right\rbrace. 
            \]
            This method is also memoryless.     
        \end{exmp}
        Other methods of $\GAfirst$ include Conjugate Gradient, Quasi-Newton, and Gradient Descent with Momentum. 
    \subsection{Lower Complexity Bounds for $L$-Lipschitz Smooth Function}
        The following is Nesterov \cite[Thm 2.1.7]{nesterov_lecture_2018}. 
        \begin{theorem}[Nesterov's Claim of Lower Bound]\label{thm:nesterov_lower_bnd}
            For any $1\le k \le 1/2(n - 1)$, for all $x^{(0)}\in \mathbb R^n$, there exists a Lipschitz smooth convex function in $\mathbb R^n$ such that for all algorithm from $\GAfirst$, we have the lower bound for the optimality gap for the function values and its iterates: 
            \[
                f\left(x^{(k)}\right) - f^* \ge 
                \frac{3L \Vert x - x^*\Vert^2}{32(k + 1)^2}, 
                \quad \Vert x^{(k)} - x^*\Vert^2 \ge \frac{1}{8} \Vert x^{(0)} - x^*\Vert^2.     
            \]
            Where $x^*$ is the minimizer of $f$, so that $f(x^*) = \inf_{x}f(x)$. 
        \end{theorem}
        \begin{remark}
            We emphasize that in \hyperref[thm:nesterov_lower_bnd]{theorem \ref*{thm:nesterov_lower_bnd}} fixes each $k$ and finds a function $f$ such that the lower bound applies at the $k$-th iterations. 
            Mathematically, it would mean 
            \begin{align*}
                \forall\; 1\le k \le \frac{n + 1}{2}, x^{(0)} \in \mathbb R^n\; 
                \exists f \in \mathcal F^{1, 1}_L \text{ s.t: }
                \min_{A\in \GAfirst} 
                \left\lbrace
                    f\left(A_f^k x^{(0)}\right)
                \right\rbrace - f^* 
                &\ge 
                \frac{3L \Vert x^{(0)} - x^*\Vert^2}{32(k + 1)^2}
                \\
                \forall\; 1\le k \le \frac{n + 1}{2}, x^{(0)} \in \mathbb R^n
                \quad
                \max_{f\in \mathcal F_L^{1,1}}
                \min_{A\in \GAfirst} 
                \left\lbrace
                    f\left(A_f^k x^{(0)}\right) - f^*  
                \right\rbrace 
                &\ge 
                \frac{3L \Vert x^{(0)} - x^*\Vert^2}{32(k + 1)^2}
                \\
                \forall
                x^{(0)} \in \mathbb R^n \quad 
                \min_{1 \le k \le 1/2 (n + 1)}
                \max_{f\in \mathcal F_L^{1, 1}}
                \min_{ A \in \GAfirst}
                \left\lbrace
                    f\left(A_f^kx^{(0)}\right) - f^*
                \right\rbrace
                & \ge 
                \frac{3L \Vert x^{(0)} - x^*\Vert^2}{32(1/2(n + 1))^2}, 
            \end{align*}
            A function $f_k$ provides the lower bound for fixed $1 \le k \le (n+ 1)/2$. 
            $k$ parameterized $f_k$, which Nesterov did in his proof. 
            We emphasize that $f_k$ is different depending on what $k$ is. 
            In addition, observe that minimizer $x^*$ is assumed to exist. 
            We believe that the theorem is generalizable to infinite dimensional Hilbert spaces. 
        \end{remark}
        We now quote Walkington \cite[theorem 2.4]{noel_nesterovs_nodate}
        \begin{theorem}[Walkington's Claim of Lower Bound]\label{thm:walkington_lowerbound}
            Let $X$ be an infinite-dimensional Hilbert Space and set $x^{(0)} =\mathbf 0$. There exists a convex function $f: X\mapsto \mathbb R$ with Lipschitz gradient and minimum $f(x_*) > -\infty$ such that for any sequence satisfying 
            \begin{align*}
                x_{i + 1}\in \text{Span}\left\lbrace
                    \nabla f(x^{(0)}), \nabla f(x^{(1)}), \cdots, \nabla f(x^{(i)})
                \right\rbrace, 
                \quad i = 0, 1, 2, \cdots, 
            \end{align*}
            there holds 
            \begin{align*}
                \min_{1\le i \le n}
                f(x_i) - f(x_*) \ge 
                \frac{3L\Vert x_1 - x_*\Vert^2}{32(n + 1)^2}, 
            \end{align*}
            where $L$ is the Lipschitz constant of the gradient. 
        \end{theorem}
        \begin{remark}
            \hyperref[thm:walkington_lowerbound]{Theorem \ref*{thm:walkington_lowerbound}} and 
            \hyperref[thm:nesterov_lower_bnd]{theorem \ref*{thm:nesterov_lower_bnd}} is completely different. 
            The former claims there exists a single function from $\mathcal F_{L}^{1,1}(\mathcal H)$ introduces the lower bound for all values of $k$, and all algorithms from $\GAfirst$, but the latter didn't claim that. 
            The difference would remain in infinite dimension Hilbert space if we were to generalize \hyperref[thm:nesterov_lower_bnd]{thorem \ref*{thm:nesterov_lower_bnd}}. 
            There is no proof after Walkington's claim; we can't know if he had his way of proving the latter claim. 
            It makes us think it is likely a missed detail in his writing. 
        \end{remark}
    \subsection{Discussion}
        Walkington cited Bubeck \cite[thm 3.14]{bubeck_convex_2015}, and Nesterov's old 2004 book\cite{nesterov_introductory_2004} for the lower bound claim. 
        Bubeck has the correct claim, and it's the same as Nesterov. 
        Attouch's claim in \cite[thm 1]{attouch_rate_2016} doesn't contradict \hyperref[thm:nesterov_lower_bnd]{theorem \ref*{thm:nesterov_lower_bnd}} because he fixed function $\Phi$ function and the existence of minimizer $x^*$ is not assumed. 
        

\section{FISTA Under Strong Convexity}\label{sec:fista_strong_convexity}
    In this section, we show several claims on the convergence proof of the algorithm of V-FISTA. 
    Beck \cite[10.7.7]{beck_first-order_nodate} inspires the works. 
    We removed one identity from their proof to reveal more transparency and interpretability. 
    The original author intends to convince the reader with as few words as possible. 
    We intend to educate and share thoughts. 
    We present the essential claims here to expedite understanding of the big picture. 
    The proofs with details are in the appendix. 
    \subsection{Setting up the Stage}
        Starting with \hyperref[alg:generic_FISTA]{algorithm \ref*{alg:generic_FISTA}}, $\theta_k = (t_k - 1)/(t_k + 1)$. 
        We consider $F = g + h$, $g$ is Lipschitz smooth with constant $L$, and strongly convex with constant $\sigma$. 
        $h$ is convex. 
        The parameters, $\theta_k$, and $t_k$, will be determined as we review the proof. 
        $t_{0} = 1$ is the base case for $t_k$ sequence; it represents the fact that there are no accelerations on the first step of the algorithm; its value depends on what we want it to be. 
        \par
        Here is a list of quantities we constructed for a better exposition. 
        \begin{itemize}
            \item [1.] $s^{(k)} = x^{(k)} - x^{(k - 1)}$, the velocity vector,
            for all $k\ge 1$. 
            \item [2.] $e^{(k)} = x^{(k)} - \bar x$, the error vector at the kth iteration, for $k \ge 0$. 
            \item [3.] $\theta_k = (t_k -1)/(t_k + 1)$, which is the momentum step size. 
            \item [4.] $u^{(k)} = \bar x + t_{k}(x^{(k - 1)} - x^{(k)}) - x^{(k - 1)}$, the error term extrapolated with the velocity. We take $u^{(0)} = \bar x - x^{(0)}$. 
            \item [5.] $\delta_k = f(x^{(k)}) - f_{\text{opt}}$, with $f_{\text{opt}} = f(\bar x) = \inf_x f(x)$. 
            \item [6.] The quantity $R_k$ plays a crucial role in the proof; it's
            $$
                R_k = 
                \frac{\sigma(t_{k + 1} - 1)}{2}
                \left\Vert
                    x^{(k)} - \bar x
                \right\Vert^2 
                - 
                \frac{L - \sigma}{2}
                \left\Vert
                    \bar x + t_{k + 1}\left( x^{(k)} - y^{(k)}\right) - x^{(k)}
                \right\Vert^2
            $$
            \item [7.] $\kappa = L/\sigma$, the condition number, it would be that $\kappa \ge 0$. 
            \item [8.] $q = \sigma/L$, the reciprical of the condition number. It would be that $q \in (0, 1)$. 
        \end{itemize}
    \subsection{Convergence Claim}
        We proposed the following alternative for proving the convergence rate of V-FISTA. 
        \begin{lemma}\label{lemma:convergence-prep}
            If there exists a sequence $(t_k)_{k\in \mathbb N}, C_k$ such that 
            \begin{align}
                \begin{cases}
                    \frac{C_k}{t_{k + 1}^2} = \frac{L(1 - t^{-1}_{k + 1})}{2t_{k}^2}
                    \\
                    R_k + C_k\left\Vert
                        u^{(k)}
                    \right\Vert^2 \ge 0, 
                \end{cases}
            \end{align}
            then 
            \begin{align*}
                \delta_{k + 1}
                \le 
                \left(
                    \prod_{i = 0}^{k} (1 - t_k^{-1})
                \right)\left(
                    \delta_0 + \frac{L}{2t_0}\left\Vert
                        u^{(0)}
                    \right\Vert^2
                \right). 
            \end{align*}
        \end{lemma}
        \begin{proof}
            see \hyperref[proof:vfist-convergence-prep]{\ref*{proof:vfist-convergence-prep}}.    
        \end{proof}
        
        \begin{prop}\label{prop:vfista-generic-convergence}
            If the sequence $(t_k)_{k \ge \mathbb N}$ by $t_{k}$ satisfies 
            $t_{k + 1} = 1 + \frac{t_k^2(1 - q)}{t_k + 1}$, and $t_{k + 1} \ge t_k + 1 - t_k^2q$ and $t_k > 1$ for all $k\ge 0$, then \hyperref[lemma:convergence-prep]{lemma \ref*{lemma:convergence-prep}} stays true. 
        \end{prop}
        \begin{proof}
            See \hyperref[proof:vfista-generic-convergence]{\ref*{proof:vfista-generic-convergence}}. 
        \end{proof}
        \begin{prop}\label{prop:vfista-linear-convergence}
            The choice of $t_k = t_{k + 1} = \sqrt{L/\sigma}\; \forall k\ge 0$ makes the proposed condition in \hyperref[lemma:convergence-prep]{proposition \ref*{lemma:convergence-prep}} true. 
            Hence, the V-FISTA variant (3.) has a convergence rate bound 
            \[
                \delta_{k + 1}
                \le 
                \left(
                    \frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}
                \right)^k\left(
                    \delta_0 + \frac{L}{2t_0}\left\Vert
                        u^{(0)}
                    \right\Vert^2
                \right).     
            \]
        \end{prop}
        \begin{proof}
            From \hyperref[prop:vfista-generic-convergence]{proposition \ref*{prop:vfista-generic-convergence}} we have 
            \begin{align*}
                \frac{t_{k + 1} - 1}{t^2_k(1 - q)}
                    -
                \frac{1}{t_k + 1} &= 0
                \\
                & \triangleright\; \text{if } t_{k + 1} = t_k, \text{ then}
                \\
                \frac{t - 1}{t^2(1 - q)} &= \frac{1}{t + 1}
                \\
                t^2(1 - q) 
                &= t^2 - 1
                \\
                t^2q &= 1
                \\
                t &= \pm \sqrt{\frac{L}{\sigma}}, 
            \end{align*}
            with $t_k > 1$ it has to be $t_k = \sqrt{\frac{L}{\sigma}}$ for all $k \ge 0$. 
            With that we have $t_{k + 1} = t_{k} + 1 - t_k^2 q = t_k + 1 - 1 = t_k$, hence the inequality is also satisfied. 
        \end{proof}
    % Check out this cool Bibtext ref \cite[this]{texbook}, woooooooooah, also it's in plain style. For some mind altering psychodelic, read \hyperref[alg:mhc]{algorithm \ref*{alg:mhc}} for the experience. For some brain expanding julia code, read \hyperref[code:brain_expand]{brain expanding julia code}. 
    % \begin{algorithm}
    %     \begin{algorithmic}[t]
    %         \STATE{\textbf{Input: $X^{(t)}$}}
    %         \STATE{$Y^{(t)} \sim q (\cdot | X^{(t)})$}
    %         \STATE{
    %             $ 
    %             \rho(x, y) := 
    %             \min\left\lbrace
    %                 \frac{f(y)}{f(x)}\frac{q(x|y)}{q(y|x)}, 1
    %             \right\rbrace
    %             $ 
    %         }
    %         \STATE{
    %             $
    %             X^{(t + 1)} := 
    %             \begin{cases}
    %                 Y^{(t)} & \text{w.p}:  \rho(X^{(t)}, Y^{(t)})
    %                 \\
    %                 X^{(t)} &  \text{otherwise}
    %             \end{cases}$
    %         }
    %     \end{algorithmic}
    %     \caption{Metropolis Chain}
    %     \label{alg:mhc}
    % \end{algorithm}
    % \label{code:brain_expand}
    % \lstinputlisting[language=julia, basicstyle=\ttfamily\scriptsize,numbers=left]{Code/juliacode.jl}

\section{Adding Non-smoothness in Nesterov's Generic Method}
    \subsection{Setting up the Stage}
    


\section{Numerical Experiments}\label{sec:numerics}
    We consider a signal length of $512$, with $t_k = k$, and $t_k = 0$ for $0\le t \le 256$ and $t_k = 1$ for $257\le t\le 512$. 
    We apply \hyperref[alg:generic_FISTA]{algorithm \ref*{alg:generic_FISTA}}, variants (4.) to the TV minimization problem with initial guess $x^{(0)} = \mathbf 0$, $\theta_1 = 0$. 
    During the execution of the algorithm, we record the following quantities 
    \begin{itemize}
        \item [1.] The gradient mapping norm $\left\Vert y^{(k)} - T(y^{(k)})\right\Vert$. 
        \item [2.] The optimality gap $F(x^{(k)}) - F_{\text{opt}}$, we estimate the optimality gap by choosing the smallest $F(x^{(k)})$ to be the minimum value from all competing algorithms, for all $k\ge 0$. 
        \item [3.] The $\theta_k$, i.e., the momentum term. 
    \end{itemize}
    The algorithm terminates upon $\Vert y^{(k)} - T(y^{(k)}) \Vert \le 10^{-10}$, or, a maximum iteration threshold is hit. 
    We use the Julia programming language \cite{bezanson_julia_2017} for the numerical implementation; see the GitHub repository \href{https://github.com/iluvjava/Proximal-Gradient/tree/main/applications}{here}.
    \subsection{Subsection}
        We showcase the results of $\alpha =10$ in \hyperref[fig:results1]{figure \ref*{fig:results1}}. 
        \begin{figure}[H]
            \centering
            \subfloat[Gradient mapping norm]{
                \includegraphics*[width=8cm]{Assets/results1/grad_map_norm.png}
            }
            \subfloat[Objective value]{
                \includegraphics*[width=8cm]{Assets/results1/obj_vals.png}
            }
            \\
            \subfloat[Momentum]{
                \includegraphics*[width=8cm]{Assets/results1/momentum.png}
            }
            \subfloat[Signal Recovery]
            {
                \includegraphics*[width=8cm]{Assets/results1/recovered_signal.png}
            }
            \caption{Experiments with $\alpha = 10$, in legend, adaptive refers to our method of spectral momentum, fixed momentum refers to V-FISTA. }
            \label{fig:results1}
        \end{figure}
        Different values of $\alpha$ affect the results for recovering the signal and convergences. 
        A smaller value of $\alpha$ creates difficulty for the convergence for many methods except ISTA. 
        It also severely affects the recovered signal.
        See \hyperref[fig:results2]{figure \ref*{fig:results2}} for more information. 
        \begin{figure}[H]
            \centering
            \subfloat[Gradient mapping norm]{
                \includegraphics*[width=8cm]{Assets/results2/grad_map_norm.png}
            }
            \subfloat[Objective value]{
                \includegraphics*[width=8cm]{Assets/results2/obj_vals.png}
            }
            \\
            \subfloat[Momentum]{
                \includegraphics*[width=8cm]{Assets/results2/momentum.png}
            }
            \subfloat[Signal Recovery]
            {
                \includegraphics*[width=8cm]{Assets/results2/recovered_signal.png}
            }
            \caption{Experiments with $\alpha = 0.5$, in legend, adaptive refers to our method of spectral momentum, fixed momentum refers to V-FISTA. }
            \label{fig:results2}
        \end{figure}
        Our method of spectral momentum, described as variant (4.), has overwhelmingly fast empirical convergence results for the TV minimization problem. 
        Its effectiveness is still under investigation. 
        We call it the spectral momentum method because of the method of Spectral Adapative Stepsize. 
        This method is described in \cite[4.1]{goldstein_field_2016}, which uses the same formula to estimate the smallest eigenvalues for the Hessian. 
        However, our method is entirely different because we applied the estimation to the $\kappa$ term for the momentum on V-FISTA. 

\section*{Acknowledgement}
    Professor Shawn Wang at UBCO, who is also my advisor. 
    He had doubts about what I said and asked great questions. 
    That helps me develop a better understanding of Nesterov's lower complexity bound. 

\pagebreak        
\appendix


\section{Appendix} 
    \input{Sections/appendix.tex}



\bibliographystyle{IEEEtran}
\bibliography{refs.bib}


\end{document}