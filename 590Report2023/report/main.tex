\documentclass[]{article}
\usepackage{amsmath}
\usepackage{amsfonts} 
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{anyfontsize} % fix font size warning. 
\usepackage{url} 
\urlstyle{same} % fix wacky url links in bib entries. 
% \usepackage{minted}

% Basic Type Settings ----------------------------------------------------------
\usepackage[margin=1in,footskip=0.25in]{geometry}
\linespread{1}  % double spaced or single spaced
\usepackage[fontsize=11pt]{fontsize}
\usepackage{authblk}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}       % Theorem counter global 
\newtheorem{prop}{Proposition}[section]  % proposition counter is section
\newtheorem{lemma}{Lemma}[subsection]  % lemma counter is subsection
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}[subsection]
{
    % \theoremstyle{plain}
    \newtheorem{assumption}{Assumption}
}
\numberwithin{equation}{subsection}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}
\usepackage[final]{graphicx}
\usepackage{listings}
\usepackage{courier}
\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\newcommand{\indep}{\perp \!\!\! \perp}
\usepackage{wrapfig}
\graphicspath{{.}}
\usepackage{fancyvrb}

%%
%% Julia definition (c) 2014 Jubobs
%%
\usepackage[T1]{fontenc}
\usepackage{beramono}
\usepackage[usenames,dvipsnames]{xcolor}
\lstdefinelanguage{Julia}%
  {morekeywords={abstract,break,case,catch,const,continue,do, else, elseif,%
      end, export, false, for, function, immutable, import, importall, if, in,%
      macro, module, otherwise, quote, return, switch, true, try, type, typealias,%
      using, while},%
   sensitive=true,%
   alsoother={$},%
   morecomment=[l]\#,%
   morecomment=[n]{\#=}{=\#},%
   morestring=[s]{"}{"},%
   morestring=[m]{'}{'},%
}[keywords,comments,strings]%
\lstset{%
    language         = Julia,
    basicstyle       = \ttfamily,
    keywordstyle     = \bfseries\color{blue},
    stringstyle      = \color{magenta},
    commentstyle     = \color{ForestGreen},
    showstringspaces = false,
}

\title{MATH 590 2023 FALL REPORT}
\author{HONGDA LI}

\begin{document}
\maketitle

\begin{abstract}
    In this paper we review the paper written by Walkington \cite{noel_nesterovs_nodate} on the topic of proximal gradient with Nesterov accelerations. 
    We compare the performance of FISTA method and some of its variants with numerical experiments on the total variation minimization problem, in addition we propose a heuristic estimation of strong convexity parameter and demonstrate that it converges faster when applied. 
    We give literature review on the frontier theoretical development on the FISTA algorithm. 
    We correct  one misconception occured in Walkington \cite{noel_nesterovs_nodate} regarding Nesterov's proof of lower bound on the optimality of first order algorithms.  
    We present a better proof of linear convergence of FISTA under strong convexity assumption from Beck \cite[theorem 10.7.7]{beck_first-order_nodate} by eliminating an identity used in the their proof. 
\end{abstract}


\section{Preliminaries}
    In this section, We present and model the problem of denoising one dimension signal. 
    The dual objective function of the problem derived in this section motivates the use of Accelerated Proximal Gradient with smooth, non-sooth composite objective function. 
    The following content are mostly summarized from Walkington \cite{noel_nesterovs_nodate}, section 1, and section 4. They are supplemented by my own writings. 

    \subsection{Signal Denoising in One Dimension}
        Let a one dimensional singal be $u: \mathbb [0, 1]\mapsto \mathbb R$ and $u$ experiences absolute continuity. 
        This class of absolutely continuous function can model discrete signal because digital signal are piecewise constant. 
        Let $\hat u$ denotes an observation of $u$ corrupted by noise. 
        The denoised signal is the minimizer of f(u),
        \[
            f(u) = \int_0^1 \frac{1}{2} 
            (u - \hat u)^2 + \alpha |u'|dt. 
        \]
        A practical approach on modern computing devices would necessitate discritization of the integral. 
        We use trapzidal rule and second order forward difference for the derivative. 
        Let $\hat u \in \mathbb R^{N+1}$, a vector in the form of $\hat u = [\hat u_0\; \cdots \; \hat u_{N}]$, let $t_0<  \cdots <t_N$ be a sequence of time corresponded to each observation of $\hat u_i$. 
        The time intervals are $h_i = t_{i} - t_{i-1}$ for $i=1, \cdots, N$, not necessarily equally spaced, hence the formulation below is slightly more general than Walkington\cite{noel_nesterovs_nodate}. 
        We derive the approximation of the integral by doing
        \begin{align*}
            & \text{Denote } s_i = u_i - \hat u_i, 
            \\
            \frac{1}{2}\int_{0}^{1} (u - \hat u)^2 dt + 
            \alpha \int_0^1 |u'| dt
            &\approx
            \frac{1}{2}
            \sum_{i = 0}^{N}
            \left(
                \frac{s_i^2 + s_{i + 1}^2}{2}
            \right)h_{i + 1}
            + 
            \alpha
            \sum_{i = 1}^{N}
            \left|
                \frac{u_{i} - u_{i - 1}}{h_{i + 1}}
            \right|
            \\
            & \triangleright\; \text{let } 
            C\in \mathbb R^{N\times (N + 1)} \text{ be upper bi-diagonal with }(1, -1)
            \\
            &= \frac{1}{2}\left(
                \frac{s_0^2h_1}{2} + \frac{s_N^2h_N}{2}
                + 
                \sum_{i = 1}^{N - 1}s_i^2 h_i
            \right) + \alpha\Vert Cu\Vert_1
            \\
            & 
            \triangleright \; \text{using } D\in \mathbb R^{N \times (N + 1)},
            \\
            & \triangleright\; D := \text{diag}(h_1/2, h_1, h_2, \cdots, h_N, h_N/2)
            \\
            &= 
            \frac{1}{2}\langle u - \hat u, D(u - \hat u)\rangle + \alpha \Vert Cu\Vert_1. 
        \end{align*}
    The above formulation suggests smooth, non-smooth additive composite objective for $f(u)$. 
    This type of optimization method can be solved via the Proximal Gradient method and its variants. 
    Unfortunately the non-smooth part $\alpha\Vert Cu\Vert_1$ presents computational difficulty if matrix $C$ is unfriendly for proximal resolvent operator. 
    One way to bypass the difficulty involves reformulating with $p = Cu$, and solve the dual problem. 
    \subsection*{Dual Reformulation}
        Let $p = Cu$, $C\in \mathbb R^{(N + 1)\times N}$ with $D \in \mathbb R^{(N + 1)\times (N + 1)}$, we reformulate it into 
        \[
            \min_{u\in \mathbb R^{N + 1}}     
            \left\lbrace
                \left.
                    \underbrace{\frac{1}{2}\langle (u - \hat u), D(u - \hat u)}_{f(u)}\rangle 
                    + 
                    \underbrace{\alpha \Vert p\Vert_1}_{h(p)}
                \right| 
                p = Cu
            \right\rbrace, 
        \]
        producing Lagrangian of the form 
        \[
            \mathcal L((u, p), \lambda) = 
            f(u) + h(p) + \langle \lambda, p - Cu\rangle. 
        \]
        The dual is
        \begin{align*}
            - g(\lambda) &:= \inf_{(u, p)\in \mathbb R^{N + 1}\times \mathbb R^N}
            \left\lbrace
                \mathcal L{(u, p), \lambda}
            \right\rbrace
            \\
            &= \inf_{(u, p)\in \mathbb R^{N + 1}\times \mathbb R^N}
            \left\lbrace
                f(u) + h(p) + \langle \lambda, p - Cu\rangle
            \right\rbrace
            \\
            &= 
            -f^\star (-C^T\lambda) - h^\star(p). 
        \end{align*}
        With the assumption that $D$ is positive definite, we have 
        \[
            - g(\lambda) = -\frac{1}{2}    \Vert C^T\lambda\Vert^2_{D^{-1}} - 
            \langle \hat u, C^T \lambda\rangle - 
            \delta_{[-\alpha, \alpha]^N}(p). 
        \]
        Observe that the above admit hyper box indicator function that makes the resolvent friendlier because proximal operator of indicator is projection, in the case of projecting onto hyper box, the operator is simple. 
        Given dual variable $\lambda$, primal is obtained by 
        \begin{align*}
            u &= \text{argmin}_u \mathcal L((u, p), \lambda) 
            \\
            \partial_u \mathcal L((u, p), \lambda) &= D(u - \hat u) - C^T\lambda = \mathbf 0
            \\
            \implies u &= \hat u + D^{-1}C^T\lambda. 
        \end{align*}
        At this point, we had a formulation such that, solving $-g(u)$ is an easy task with the smooth non-smooth additive objective, and obtaining the primal solution is simple as well since $D^{-1}$ is a diagonal matrix. 
    \subsection{FISTA has Worse Convergence Guarantee for Strongly Convex Objectives}
        The dual objective for a total variation minimization problem is a strongly convex and Lipschitz smooth function because of the norm induced by the positive definite matrix $D^{-1}$. 
        It's in a form where FISTA propsed by \cite{beck_fast_2009-1} can solve with a convergence rate of $\mathcal O(1/k^2)$ on the objective value of the function. 
        However, highlighted in Walkington\cite{noel_nesterovs_nodate}, the proximal gradient method without acceleration achieves $\mathcal O((1 - 1/\kappa)^k)$ convergence rate. 
        Which is faster.
        The parameter $\kappa$ is the condition number, in this case it would be $L/\alpha$, where $L$ is the lipschitz smooth constant of $g(u)$ and $\alpha$ is the strong convexity constant for $g(u)$. 
        We emphasize here that for the class of strongly convex objectives, Proximal Gradient without acceleration has a better theoretical convergence results than the accelerated version. 
        This surprising facts hints at a fundamental difference between methods with, and without Nesterov's acceleration. 
        It sparks the discussion in this paper on the variants of FISTA in hope of providing some insights on the reasons for Nesterove's momentum based method's inability to adapt the convergence rate with objective has strong convexity. 
        For the terminologies, we use FISTA to specifically refers to the proximal gradient method presented by Beck and Teboulle\cite{beck_fast_2009-1}. We use Accelerated Proximal Gradient method (APG) to refer to the class of first order acceleration algorithms developed/inspired from FISTA. 

    \subsection{Outline of the Paper}
        \hyperref[sec:Literatures]{Section \ref*{sec:Literatures}} consists of 3 parts. 
        Reviewing of iteratures on the problem of total variation minimization for image/signal denoising and deblurring is the first part. 
        Presenting FISTA and its variants is the second part. 
        Reviewing the algorithmic tricks and improvements applied to the APG is the third parts. 
        \hyperref[sec:optimal_lower_bound]{Section \ref*{sec:optimal_lower_bound}} addresses a mistake made in Walkington's writing \cite[theorem 2.4]{noel_nesterovs_nodate}. 
        We will talk about what a first order method is and the fact that the lower complexity bound on the objective value and iterates for a fixed iteration, is achieved by a different function. 
        We discuss how this omitting the details of this theorem creates potential misconceptions of other frontier research ideas. 
        \hyperref[sec:fista_strong_convexity]{Section \ref*{sec:fista_strong_convexity}} presents a proof that I adapted from Amir Beck's writing \cite[theorem 10.7.7]{beck_first-order_nodate}. 
        The proof is slightly more general and it removed an equality to strengthens interpretibility and generality. 
        \hyperref[sec:numerics]{Section \ref*{sec:numerics}} presents plots of convergence and results of applying variants of Accelerated Proximal Methods to the Total Variation problem. 
        

        
        


\section{Literatures Review}\label{sec:Literatures}
    \subsection{Total Variation Minimizations}
        Total Variation (TV) minimization method was introduced by Rudin-Osher and Fatemi in \cite{rudin_nonlinear_1992}. 
        They pioneer the theories of TV minimizations by solving PDE. 
        They discussed the empirical observation that L1 regularizations term produces sharper images. 
        Walkington \cite{noel_nesterovs_nodate} a basic formulation of one dimensional signal denoising. 
        However it's important to keep in mind that this is a problem that motivates a variety of modern computational methods and theories. 
        We will list some of them for context. 
        Goldstein, et al in \cite[3.2.1]{goldstein_field_2016} showcased the dual reformulation of a 2D signal recovery with $\Vert \nabla u\Vert$ as the regularizations term. 
        We note that this norm is without the squared. 
        A more hardcore, detailed coverage of reformulating the dual with a L1 penalty terms for 2D signal recovery is in \cite{beck_fast_2009}. 
        For a full survey of state of arts computational methods applied to TV minimizations see Chambolle \cite{chambolle_introduction_2016}. 
        For a detailed exposition of mathematical theories regarding variational analysis on different type of TV problems and statistical inferences based interpretations of TV regularization term, consult the work by Chambolle et al\cite{fornasier_introduction_2010}. 
        For frontier work of applying non-convex penalty term and its theoretical guarantee consult \cite{an_enhanced_2023}, \cite{an_springback_2022}. 
    \subsection*{Variants of FISTA}
        For different Variants of FISTA, we specifically refer to first order acceleration method based on Nesterov's Framework, adhering to Nesterov's lectures \cite{nesterov_lecture_2018} and Beck's book \cite[chapter 10]{beck_first-order_nodate}. 
        Nesterov's lectures focuses on establishing the most general frameworks for this type of methods. 
        Nesterov approach doesn't handle function that is non-smooth, at least not directly. 
        However, in his lectures, he derived a generic accelerated gradient algorithm (2.2.7) based on the idea of accelerating sequences. 

        
        Nesterov's derivation of the linear convergence rate, and sub-linear convergences rate, were completed in theorem 2.2.2, based on the results of lemma 2.2.4. 
        We emphasize that in Nesterov's writing, his derivation of linear and sub-linear convergence of the function objective were done in lemma 2.2.4, without the assumption of a minimizer, and for Lipschitz smooth function with and without strong convexity. 
        Amir Beck's book had the algorithm V-FISTA presented in 10.7.7, it's an algorithm that is exactly the same as Nesterov's algorithm: 2.2.22, with the addition of a proximal operator. 
        \begin{algorithm}\label{alg:V-FISTA}
        \begin{algorithmic}[1]
            \STATE{\textbf{Input: }($f, g, x^{(0)}$)}
        \end{algorithmic}\caption{V-FISTA}
        \end{algorithm}

        



\section{Nesterov's Lower Bound Clarified}\label{sec:optimal_lower_bound}
    \subsection{title}

\section{FISTA Under Strong Convexity}\label{sec:fista_strong_convexity}
    
    \subsection{Subsection}

    % Check out this cool Bibtext ref \cite[this]{texbook}, woooooooooah, also it's in plain style. For some mind altering psychodelic, read \hyperref[alg:mhc]{algorithm \ref*{alg:mhc}} for the experience. For some brain expanding julia code, read \hyperref[code:brain_expand]{brain expanding julia code}. 
    % \begin{algorithm}
    %     \begin{algorithmic}[t]
    %         \STATE{\textbf{Input: $X^{(t)}$}}
    %         \STATE{$Y^{(t)} \sim q (\cdot | X^{(t)})$}
    %         \STATE{
    %             $ 
    %             \rho(x, y) := 
    %             \min\left\lbrace
    %                 \frac{f(y)}{f(x)}\frac{q(x|y)}{q(y|x)}, 1
    %             \right\rbrace
    %             $ 
    %         }
    %         \STATE{
    %             $
    %             X^{(t + 1)} := 
    %             \begin{cases}
    %                 Y^{(t)} & \text{w.p}:  \rho(X^{(t)}, Y^{(t)})
    %                 \\
    %                 X^{(t)} &  \text{otherwise}
    %             \end{cases}$
    %         }
    %     \end{algorithmic}
    %     \caption{Metropolis Chain}
    %     \label{alg:mhc}
    % \end{algorithm}
    % \label{code:brain_expand}
    % \lstinputlisting[language=julia, basicstyle=\ttfamily\scriptsize,numbers=left]{Code/juliacode.jl}

            

\section{Numerical Experiments}\label{sec:numerics}
    
\appendix

\section{Appendix} 
    \input{Sections/appendix.tex}



\bibliographystyle{IEEEtran}
\bibliography{refs.bib}


\end{document}