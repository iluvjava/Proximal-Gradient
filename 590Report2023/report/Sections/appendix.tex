\subsection{Proof for Lemma \ref*{lemma:convergence-prep}}
    For better notation we use $T$ to denote the proximal gradient operator $T_L$ appeared in \hyperref[alg:generic_FISTA]{Algorithm \ref*{alg:generic_FISTA}}. 
    We need the following lemma to start the proof. 
    See \cite[remark 10.17]{beck_first-order_nodate} for the Proximal Gradient Lemma. 
    
    \begin{lemma}[Proximal Gradient Lemma]
        Let $F = g + h$ where $h$ is convex closed and proper, $g$ is $L$-Lipschitz smooth with a constant of $L$. Let $y\in \mathbb R^n$, we define $y^+ = T(y)$, then for any $x\in \mathbb R^n$, we have: 
        $$
        \begin{aligned}
        f(x) - f(y^+) \ge \frac{L}{2}\Vert x - y^+\Vert^2 - \frac{L}{2}\Vert x - y\Vert^2 + D_g(x, y),
        \end{aligned}
        $$
        Where $D_g(x, y):= g(x) - g(y) - \langle \nabla g(y), x -y\rangle$ is the Bregman Divergence for the smooth part of the sum: $g$. 
    \end{lemma}
    The following lemma is from Nesterov's new book \cite[thm 2.1.9, (2.1.23)]{nesterov_lectures_2018}. 
    
    \begin{lemma}[Strong Convexity and the Cute Formula]
        Let $f$ be continuous differentiable and $\mu$-strongly convex on $Q\subseteq \mathbb R^n$, then for all $x, y \in Q$ and $\alpha\in [0, 1]$, we have the equivalent conditions 
        \begin{align}
            \langle \nabla f(x) - \nabla f(y), x - y\rangle &\ge \mu
            \Vert x - y\Vert^2, 
            \\
            \alpha f(x) + (1 - \alpha) f(y) &\ge
            f(\alpha x + (1 - \alpha)) + 
            \frac{\mu \alpha(1 - \alpha)}{2}
            \Vert x - y\Vert^2. 
        \end{align}
    \end{lemma}

    \begin{proof}[Proof of Lemma \ref*{lemma:convergence-prep}]\label{proof:vfist-convergence-prep}
        For the sequence $(t_k)_{k\in \mathbb N\cup \{0\}}$, we consider substituting $x, y$ with:
        \begin{itemize}
            \item [1.] $x = t^{-1}_{k + 1}\bar x + (1 - t^{-1}_{k + 1})x^{(k)}, y = y^{(k)}$. 
            \item [2.] $\bar x \in \underset{x}{\text{argmin}}\; F(x)$ and $f(\bar x) = F_{\text{opt}}$. 
        \end{itemize}
        From the Proximal Gradient Lemma, using strong convexity of $g$, $D_g(x, y) \ge \frac{\sigma}{2}\Vert y - x\Vert^2$ for all $x, y$ then
        \begin{align}
            F(x) - F\circ T(y) &\ge 
            \frac{L}{2}\Vert x - T (y)\Vert^2 - \frac{L}{2}\Vert x - y\Vert^2 + D_g(x, y)
            \\
            & \ge \frac{L}{2}\Vert x - Ty\Vert^2 - \frac{L - \sigma}{2}\Vert x - y\Vert^2. 
        \end{align}\label{eqn:proxgrad_lemma}
        Expandind what we substituted into \hyperref[eqn:proxgrad_lemma]{\ref*{eqn:proxgrad_lemma}}, we can simplify and get
        \begin{align*}
            x - T(y) &=  t^{-1}_{k+1} \bar x + (1 - t^{-1}_{k+1}) x^{(k)} - T y^{(k)}
            \\
            &= t^{-1}_{k+1} \bar x + (1 - t^{-1}_{k+1}) x^{(k)} - x^{(k + 1)}
            \\
            &= t^{-1}_{k + 1}\left(
                \bar x + (t_{k+1} - 1)x^{(k)} - t_{k+1}x^{(k + 1)}
            \right)
            \\
            &= t^{-1}_{k + 1} \left(
                \bar x + t_{k+1} \left(
                    x^{(k)} - x^{(k + 1)}
                \right) - x^{(k)}
            \right), 
            \\
            x - y &= t^{-1}_{k+1} \bar x + (1 - t_{k+1}^{-1}) x^{(k)} - y^{(k)}
            \\
            &= t^{-1}_{k + 1}\left(
                \bar x + (t_{k+1} - 1)x^{(k)} - t_{k+1} y^{(k)}
            \right)
            \\
            &= t^{-1}_{k + 1}
            \left(
                \bar x + t_{k+1} \left(
                    x^{(k)} - y^{(k)}
                \right) - x^{(k)}
            \right), 
        \end{align*}
        rewriting, the RHS of \hyperref[eqn:proxgrad_lemma]{\ref*{eqn:proxgrad_lemma}} so
        \begin{align*}
            \frac{L}{2} \left\Vert
            t^{-1}_{k + 1}\left(
             \bar x + t_{k + 1} \left(
                 x^{(k)} - x^{(k + 1)}
             \right) - x^{(k)}
             \right)
            \right\Vert^2 - 
            \frac{(L - \sigma)}{2}
            \left\Vert
                t^{-1}_{k + 1}\left(
                    \bar x + t_{k + 1} \left(
                        x^{(k)} - y^{(k)}
                    \right) - x^{(k)}
                \right)
            \right\Vert^2. 
        \end{align*}
        The LHS of \hyperref[eqn:proxgrad_lemma]{\ref*{eqn:proxgrad_lemma}} can be bounded using strong convexity of $F$.
        \begin{align*}
            F(x) - F\circ T(y)
            &= F(x) - F\left(
                x^{(k + 1)}
            \right) 
            \\
            &= F\left(t_{k + 1}^{-1}x^{(k)} + (1 - t^{-1}_{k + 1})\bar x\right) - 
            F\left(x^{(k + 1)}\right)
            \\
            &\le 
            t_{k + 1}^{-1}F_{\text{opt}}
            + 
            (1 - t_{k + 1}^{-1})F\left(x^{(k)}\right)
            - 
            \frac{\sigma}{2}t^{-1}_k\left(1 - t^{-1}_{k + 1}\right)
            \left\Vert 
                x^{(k)} - \bar x
            \right\Vert^2 
            - F\left(x^{(k + 1)}\right)
            \\
            &= 
            t_{k + 1}^{-1} \left(
                F_{\text{opt}} - F\left(x^{(k)}\right)
            \right) + F\left(x^{(k)}\right) - F\left(x^{(k + 1)} \right)
            -
            \frac{\sigma}{2}t^{-1}_{k + 1}\left(1 - t^{-1}_{k + 1}\right)
            \left\Vert 
                x^{(k)} - \bar x
            \right\Vert^2. 
            \\
        \end{align*}
        Denote $\delta_k := F\left(x^{(k)}\right) - F_{\text{opt}}$, 
        \begin{align*}
            F(x) - F\circ T(y) &\le  -t_{k + 1}^{-1}\delta_k + \delta_k  - \delta_{k + 1} 
            -
            \frac{\sigma}{2}t^{-1}_{k + 1}\left(1 - t^{-1}_{k + 1}\right)
            \left\Vert 
                x^{(k)} - \bar x
            \right\Vert^2. 
        \end{align*}
        With the above we present the full form of \hyperref[eqn:proxgrad_lemma]{\ref*{eqn:proxgrad_lemma}} so 
        \begin{align}
            & (1-t_{k + 1}^{-1})\delta_k  - \delta_{k + 1} 
            -
            \frac{\sigma}{2}t^{-1}_{k + 1}\left(1 - t^{-1}_{k + 1}\right)
            \left\Vert 
                x^{(k)} - \bar x
            \right\Vert^2 
            \nonumber
            \\
            &\quad\ge \frac{L}{2} \left\Vert
               t^{-1}_{k + 1}\left(
                \bar x + t_{k + 1} \left(
                    x^{(k)} - x^{(k + 1)}
                \right) - x^{(k)}
                \right)
            \right\Vert^2 - 
            \frac{(L - \sigma)}{2}
            \left\Vert
                t^{-1}_{k + 1}\left(
                    \bar x + t_{k + 1} \left(
                        x^{(k)} - y^{(k)}
                    \right) - x^{(k)}
                \right)
            \right\Vert^2. 
            \nonumber
            \\
            & (t_{k + 1}^2 - t_{k + 1})\delta_k - t_{k + 1}^2\delta_{k + 1}
            - \frac{\sigma}{2}(t_{k + 1} - 1)
            \left\Vert
                x^{(k)} - \bar x
            \right\Vert^2 
            \nonumber
            \\
            &\quad\ge 
            \frac{L}{2} \left\Vert
                \bar x + t_{k + 1} \left(
                    x^{(k)} - x^{(k + 1)}
                \right) - x^{(k)}
            \right\Vert^2 - 
            \frac{(L - \sigma)}{2}
            \left\Vert
                \bar x + t_{k + 1} \left(
                    x^{(k)} - y^{(k)}
                \right) - x^{(k)}
            \right\Vert^2
            \nonumber
            \\
            & 
            (t_{k + 1}^2 - t_{k + 1})\delta_k - t_{k + 1}^2\delta_{k + 1} 
            \underbrace{
                - 
                \frac{\sigma(t_{k + 1} - 1)}{2}
                \left\Vert
                    x^{(k)} - \bar x
                \right\Vert^2 
                + 
                \frac{L - \sigma}{2}
                \left\Vert
                    \bar x + t_{k + 1}\left( x^{(k)} - y^{(k)}\right) - x^{(k)}
                \right\Vert^2
            }_{-R_k}
            \nonumber
            \\
            &\quad \ge 
            \frac{L}{2}\left\Vert 
                \bar x + t_{k + 1}
                \left(x^{(k)} - x^{(k + 1)}\right) - x^{(k)}
            \right\Vert^2. 
            \label{eqn:full_pg}
        \end{align}
        Recall the quantities from \hyperref[sec:fista_strong_convexity]{section \ref*{sec:fista_strong_convexity}}, and from the algorithm
        \begin{align*}
                y^{(k)} &= x^{(k)} + \theta_k(x^{(k)} - x^{(k - 1)})
                \\
                y^{(k)} - x^{(k)} &= \theta_k(x^{(k)} - x^{(k - 1)}) = \theta_k s^{(k)}. 
        \end{align*}
        Simplifying, 
        \begin{align*}
            &\quad \frac{L - \sigma}{2}
            \left\Vert
                \bar x + t_{k + 1}\left( x^{(k)} - y^{(k)}\right) - x^{(k)}
            \right\Vert^2
            \\
            &=  \frac{L - \sigma}{2}\left\Vert
                \bar x - x^{(k)} - t_{k + 1}\theta_k s^{(k)}
            \right\Vert^2
            \\
            &= \frac{L - \sigma}{2}\left\Vert e^{(k)} + t_{k + 1}\theta_k s^{(k)}\right\Vert^2, 
        \end{align*}
        Observe that $u^{(k + 1)} = \bar x + t_{k + 1}(x^{(k)} - x^{(k +1)}) - x^{(k)}$, it is in the norm on the RHS of \hyperref[eqn:full_pg]{\ref*{eqn:full_pg}}. 
        $u^{(k)}$ has representation by $e^{(k)}, x^{(k)}$. 
        \begin{align*}
            u^{(k)} &= \bar x + t_{k}\left(x^{(k - 1)} - x^{(k)} \right) - x^{(k - 1)}
            \\
            &= \bar x + (t_{k} - 1)\left(
                x^{(k - 1)} - x^{(k)}
            \right) + 
            \left(
                x^{(k - 1)} - x^{(k)}
            \right) - x^{(k - 1)}
            \\
            &= \bar x + 
            (t_{k} - 1)\left(x^{(k - 1)} - x^{(k)}\right)
            - x^{(k)}
            \\
            &= - e^{(k)} - (t_{k} - 1)s^{(k)}, 
        \end{align*}
        with these simplifications, we will be able to write down \hyperref[eqn:full_pg]{\ref*{eqn:full_pg}} as 
        \begin{align}
            R_k &=  
            \frac{\sigma(t_{k + 1} - 1)}{2}
            \left\Vert e^{(k)}\right\Vert^2
            - 
            \frac{L - \sigma}{2}\left\Vert e^{(k)} + t_{k + 1}\theta_k s^{(k)}\right\Vert^2, 
            \\
            t_{k + 1}(t_{k + 1} - 1)\delta_k - R_k
            &\ge 
            t_{k + 1}^2\delta_{k + 1} + \frac{L}{2}\left\Vert u^{(k + 1)}\right\Vert^2. 
        \end{align}
        We are now prepared for deriving a bound on the convergence rate of the algorithm. 
        \begin{align*}
            t_{k + 1}^2\delta_{k + 1}  + 
            \frac{L}{2}\left\Vert
                u^{(k + 1)}
            \right\Vert^2 
            - 
            C_k
            \left\Vert
                u^{(k)}
            \right\Vert^2
            &\le
            t_{k + 1}(t_{k + 1} - 1)\delta_k - R_k
            -
            C_k
            \left\Vert
                u^{(k)}
            \right\Vert^2
            \\
            t_{k + 1}^2\delta_{k + 1} + \frac{L}{2}\left\Vert
                u^{(k + 1)}
            \right\Vert^2 
            - C_k
            \left\Vert
                u^{(k)}
            \right\Vert^2
            & \le 
            t_{k + 1}(t_{k + 1} - 1)\delta_k
            \\
            \delta_{k + 1}
            &\le 
            (1 - t_{k + 1}^{-1}) \delta_k + \frac{C_k}{t_{k + 1}^{2}}\left\Vert
                u^{(k)}
            \right\Vert^2 -
            \frac{L}{2t^2_{k + 1}}\left\Vert
                u^{(k + 1)}
            \right\Vert^2. 
        \end{align*}
        Going from the first inequality to the second we used $R_k + C_k\left\Vert u^{(k)}\right\Vert\ge 0$ in \hyperref[lemma:convergence-prep]{lemma \ref*{lemma:convergence-prep}}
        Going from the second inequality to the third we devided both sides by $t_{k + 1}^2$ with rearrangement. 
        Let's use  $\frac{C_k}{t_{k + 1}^2} = \frac{L(1 - t^{-1}_{k + 1})}{2t_{k}^2}$ from \hyperref[lemma:convergence-prep]{lemma \ref*{lemma:convergence-prep}} so
        \begin{align*}
            \delta_{k + 1} &\le 
            (1 - t_{k + 1}^{-1})\delta_k + 
            \frac{L(1 - t_{k + 1}^{-1})}{2t_{k}^2} \left\Vert
                u^{(k)}
            \right\Vert^2 - \frac{L}{2t_{k + 1}^2}\left\Vert u^{(k + 1)} \right\Vert^2
            \\
            \delta_{k + 1} &\le 
            (1 - t_{k + 1}^{-1})\left(
                \delta_k + \frac{L}{2t_{k}^2}\left\Vert
                    u^{(k)}
                \right\Vert^2
            \right) 
            - \frac{L}{2t_{k + 1}^2}\left\Vert u^{(k + 1)}\right\Vert^2
            \\
            & \triangleright \text{ unrolling recursion yield }
            \\
            \delta_{k + 1}
            &\le 
            \left(
                \prod_{i = 0}^{k} \left(
                    1 - t_k^{-1}
                \right)
            \right)\left(
                \delta_0 + \frac{L}{2t_0^2}\left\Vert
                    u^{(0)}
                \right\Vert^2
            \right). 
        \end{align*}
        We are done. 
    \end{proof}

\subsection{Proof for Proposition \ref*{prop:vfista-generic-convergence}}
    \begin{proof}\label{proof:vfista-generic-convergence}
        Observe that condition from \hyperref[lemma:convergence-prep]{lemma \ref*{lemma:convergence-prep}} implies 
        \begin{align*}
            R_k + C_k\left\Vert
            u^{(k)}
            \right\Vert^2 &\ge 0
            \\
            \frac{\sigma(t_{k + 1} - 1)}{2}
            \left\Vert e^{(k)}\right\Vert^2
            - 
            \frac{L - \sigma}{2}
            \left\Vert e^{(k)} + 
                t_{k + 1}\theta_k s^{(k)}
            \right\Vert^2 
            + 
            \frac{Lt_{k + 1}(t_{k + 1} - 1)}{2t_k^2} 
            \left\Vert u^{(k)}\right\Vert^2 &\ge 0
            \\
            \frac{\sigma(t_{k +1} - 1)}{L - \sigma}
            \left\Vert e^{(k)}\right\Vert^2
            - 
            \left\Vert e^{(k)} + 
                t_{k + 1}\theta_k s^{(k)}
            \right\Vert^2 
            + 
            \frac{Lt_{k + 1}(t_{k + 1} - 1)}{t^2_k(L - \sigma)}
            \left\Vert 
                e^{(k)} + (t_{k} - 1)s^{(k)}
            \right\Vert^2
            &\ge  0
        \end{align*}
        From the first to the second line, divide $\frac{L - \sigma}{2}$, From the second to the thrid line recall $u^{(k)} = -e^{(k)} - (t_{k} - 1)s^{(k)}$.
        Since the vector quantities $e^{(k)}, s^{(k)}$ share the same superscript, we may ignore it.
        Expanding the expression would yield quantities $\langle s, e\rangle, \Vert s\Vert^2, \Vert e\Vert^2$, we list them for each term all 
        \begin{align*}
            & 
            - \Vert e + t_{k + 1}\theta_k s\Vert^2
            = - \left(
                \Vert e\Vert^2 + t_{k + 1}^2\theta_k^2 \Vert s\Vert^2 + 
                2\theta_k t_{k + 1}\langle e, s\rangle
            \right), 
            \\
            & 
            \frac{Lt_{k + 1}(t_{k + 1} - 1)}{t_k^2(L - \sigma)} \Vert 
                e + (t_k - 1)s
            \Vert^2
            = 
            \frac{Lt_{k + 1}(t_{k + 1} - 1)}{t_k^2(L - \sigma)} 
            \left(
                \Vert e\Vert^2 + (t_k -1)^2 \Vert s\Vert^2 + 2(t_k - 1)\langle e, s\rangle
            \right), 
            \\
            & \frac{\sigma (t_{k + 1} - 1)}{L - \sigma}
            \Vert e\Vert^2. 
        \end{align*}
        Grouping each of the terms $\Vert e\Vert^2, \Vert s\Vert^2, \langle e, s\rangle$, we compute their coefficients with $q = \sigma/L$, 
        \begin{align*}
            \Vert e\Vert^2 \text{ has: } &
            \quad 
            \frac{\sigma (t_{k + 1} - 1)}{L - \sigma} 
            + 
            \frac{Lt_{k + 1}(t_{k + 1} - 1)}{t_k^2(L - \sigma)} - 1
            \nonumber
            \\
            & = (t_{k + 1} - 1)\left(
                \frac{q}{1 - q} + \frac{t_{k + 1}}{t_k^2(1 - q)}
            \right) - 1
            \nonumber
            \\
            &= 
            \frac{t_{k + 1} - 1}{1 - q}\left(
                q + \frac{t_{k + 1}}{t_k^2}
            \right) - 1
            \nonumber
            \\
            \Vert s\Vert^2 \text{ has: } & 
            \quad 
            \frac{Lt_{k + 1}(t_{k + 1} - 1)}{t_k^2 (L - \sigma)}(t_k - 1)^2
            - t_{k + 1}^2 \theta_k^2
            \nonumber
            \\
            &= 
            \frac{t_{k + 1}(t_{k + 1} - 1)}{t_k^2 (1 - q)}(t_k - 1)^2
            - t_{k + 1}^2\left(
                \frac{t_k - 1}{t_k + 1}
            \right)^2
            \nonumber
            \\
            &= 
            (t_k - 1)^2
            \left(
                \frac{t_{k + 1}(t_{k + 1} - 1)}{t_k^2(1 - q)}
                - 
                \frac{t_{k + 1}^2}{(t_k + 1)^2}
            \right)
            \nonumber
            \\
            \langle s, e\rangle \text{ has: } &
            \quad 
            \frac{2Lt_{k + 1}(t_{k + 1} - 1)}{t_k^2(L - \sigma)}(t_k -1)
            -2\theta_kt_{k + 1}
            \nonumber
            \\
            &= 2(t_k - 1)t_{k + 1}
            \left(
                \frac{t_{k + 1} - 1}{t^2_k(1 - q)}
                -
                \frac{1}{t_k + 1}
            \right),
        \end{align*}
        To satisfy the assumption, it would be great to have the coefficients for $\langle s, e\rangle$ to be zero, and the coefficients of $\Vert e\Vert^2, \Vert s\Vert^2$ to be a positive quantities. 
        \par
        For the coefficient of $\langle s, e\rangle$ to be zero, it would imply the condition
        \begin{align}
            \frac{t_{k + 1} - 1}{t^2_k(1 - q)}    
            &= 
            \frac{1}{t_k + 1} \label{eqn:sequence-cond3}
            \\
            t_{k + 1} 
            &= \frac{t_k^2(1 - q)}{t_k + 1} + 1, 
        \end{align}
        we assume that $t_k \neq 1$ for all $k\ge 0$. 
        Next, we consider the non-negativity condition for the coefficients of $\Vert e\Vert^2$, so 
        \begin{align}
            \frac{t_{k + 1} - 1}{1 - q} 
            \left(
                q + \frac{t_{k + 1}}{t_k^2}
            \right)
            -1
            &\ge 0
            , \quad\text{using }
            \frac{t_{k + 1}-1}{1 - q}
            = \frac{t_k^2}{t_k + 1}
            \nonumber
            \\
            \frac{t_k^2}{t_k + 1}\left(
                q + \frac{t_{k + 1}}{t_k^2} 
            \right) 
            &> 1, \quad 
            \text{using } t_k > 1, \forall k\ge0, \text{ from \hyperref[prop:vfista-generic-convergence]{proposition \ref*{prop:vfista-generic-convergence}}}, 
            \nonumber
            \\
            t_k^2 q + t_{k + 1} &\ge t_k + 1
            \nonumber
            \\
            t_{k + 1} &\ge t_k + 1 - t_k^2 q. \label{eqn:seq-cond1}
        \end{align}
        Similarly the non-negatvity constraints for $\Vert s^{(k)}\Vert^2$ would yield 
        \begin{align*}
            (t_k - 1)^2 
            \left(
                \frac{t_{k + 1}(t_{k + 1} - 1)}{t_k^2(1 - q)}
                -
                \frac{t_{k + 1}^2}{(t_k + 1)^2}
            \right) & 
            \ge 0, \text{ by }
            \frac{t_{k + 1} - 1}{t_k^2(1 - q)}
            = 
            \frac{1}{t_k + 1}
            \\
            (t_k - 1)^2 \left(
                \frac{t_{k +1}}{t_k + 1} - 
                \frac{t_{k + 1}^2}{(t_k +1)^2} 
            \right)
            &\ge 0,  \; (t_k-1) > 0, \text{ so we devided by } (t_k - 1)^2
            \\
            \frac{t_{k + 1}}{t_k + 1} &\ge 
            \frac{t_{k + 1}^2}{(t_k + 1)^2}
            \\
            \frac{1}{t_k + 1} & \ge 
            \frac{t_{k + 1}}{(t_k + 1)^2}
            \\
            1 &\ge 
            \frac{t_{k + 1}}{t_k + 1}
            \\
            t_k + 1 &\ge t_{k + 1}. 
        \end{align*}
        Now, under the assumption of $t_k > 1$, the above condition would be redundant because \ref*{eqn:sequence-cond3} has 
        \begin{align*}
            \frac{t_{k + 1} - 1}{t_k^2(1 - q)}
            -
            \frac{1}{t_k + 1} &= 0
            \\
            t_{k + 1} - 1 - 
            \frac{t_k^2(1 - q)}{t_k + 1} &= 0
            \\
            t_{k + 1} &= 1 + 
            \frac{t_k^2(1 - q)}{t_k + 1}
            \\
            & \le 1 + t_k^2/(t_k + 1)
            \\
            &\le 1 + t_k^2/t_k = 1 + t_k. 
        \end{align*}
        We are done. 
        With (\ref*{eqn:seq-cond1}), (\ref*{eqn:sequence-cond3}), and $t_k > 1$ we can use the generic convergence rate stated in \hyperref[prop:vfista-generic-convergence]{Proposition \ref*{prop:vfista-generic-convergence}}. 
    \end{proof}
    


