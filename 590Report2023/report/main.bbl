\begin{thebibliography}{10}

\bibitem{ahn_understanding_2022}
{\sc K.~Ahn and S.~Sra}, {\em Understanding nesterov's acceleration via
  proximal point method}, June 2022.
\newblock arXiv:2005.08304 [cs, math].

\bibitem{alamo_gradient_2019}
{\sc T.~Alamo, P.~Krupa, and D.~Limon}, {\em Gradient {Based} {Restart}
  {FISTA}}, in 2019 {IEEE} 58th {Conference} on {Decision} and {Control}
  ({CDC}), Dec. 2019, pp.~3936--3941.
\newblock ISSN: 2576-2370.

\bibitem{an_springback_2022}
{\sc C.~An, H.-N. Wu, and X.~Yuan}, {\em The springback penalty for robust
  signal recovery}, Applied and Computational Harmonic Analysis, 61 (2022),
  pp.~319--346.
\newblock arXiv:2110.06754 [cs, math].

\bibitem{an_enhanced_2023}
\leavevmode\vrule height 2pt depth -1.6pt width 23pt, {\em Enhanced total
  variation minimization for stable image reconstruction}, Inverse Problems, 39
  (2023), p.~075005.
\newblock arXiv:2201.02979 [cs, eess, math].

\bibitem{attouch_rate_2016}
{\sc H.~Attouch and J.~Peypouquet}, {\em The rate of convergence of nesterov's
  accelerated forward-backward method is actually aaster than
  \$1/k{\textasciicircum}2\$}, SIAM Journal on Optimization, 26 (2016),
  pp.~1824--1834.
\newblock Publisher: Society for Industrial and Applied Mathematics.

\bibitem{aujol_parameter-free_2023}
{\sc J.-F. Aujol, L.~Calatroni, C.~Dossal, H.~Labarrière, and A.~Rondepierre},
  {\em Parameter-{Free} {FISTA} by adaptive restart and backtracking},
  arXiv.org,  (2023).

\bibitem{aujol_fista_2022}
{\sc J.-F. Aujol, C.~H. Dossal, H.~Labarrière, and A.~Rondepierre}, {\em
  {FISTA} restart rsing an automatic estimation of the growth parameter},
  (2022).

\bibitem{beck_first-order_nodate}
{\sc A.~Beck}, {\em First-{Order} {Methods} in {Optimization} {\textbar} {SIAM}
  {Publications} {Library}}, {MOS}-{SIAM} {Series} in {Optimization}, SIAM.

\bibitem{beck_fast_2009}
{\sc A.~Beck and M.~Teboulle}, {\em Fast gradient-based algorithms for
  constrained total variation image denoising and deblurring problems}, IEEE
  Transactions on Image Processing, 18 (2009), pp.~2419--2434.
\newblock Conference Name: IEEE Transactions on Image Processing.

\bibitem{beck_fast_2009-1}
\leavevmode\vrule height 2pt depth -1.6pt width 23pt, {\em A fast iterative
  shrinkage-thresholding algorithm for linear inverse problems}, SIAM Journal
  on Imaging Sciences, 2 (2009), pp.~183--202.

\bibitem{bezanson_julia_2017}
{\sc J.~Bezanson, A.~Edelman, S.~Karpinski, and V.~B. Shah}, {\em Julia: {A}
  {Fresh} {Approach} to {Numerical} {Computing}}, SIAM Review, 59 (2017),
  pp.~65--98.
\newblock Publisher: Society for Industrial and Applied Mathematics.

\bibitem{bubeck_convex_2015}
{\sc S.~Bubeck}, {\em Convex optimization: algorithms and complexity},  (2015).
\newblock arXiv:1405.4980 [cs, math, stat].

\bibitem{fornasier_introduction_2010}
{\sc A.~Chambolle, V.~Caselles, D.~Cremers, M.~Novaga, and T.~Pock}, {\em An
  {Introduction} to {Total} {Variation} for {Image} {Analysis}}, in Theoretical
  {Foundations} and {Numerical} {Methods} for {Sparse} {Recovery},
  M.~Fornasier, ed., DE GRUYTER, July 2010, pp.~263--340.

\bibitem{chambolle_convergence_2015}
{\sc A.~Chambolle and C.~Dossal}, {\em On the {Convergence} of the {Iterates}
  of the “{Fast} {Iterative} {Shrinkage}/{Thresholding} {Algorithm}”},
  Journal of Optimization Theory and Applications, 166 (2015), pp.~968--982.

\bibitem{chambolle_introduction_2016}
{\sc A.~Chambolle and T.~Pock}, {\em An introduction to continuous optimization
  for imaging}, Acta Numerica, 25 (2016), pp.~161--319.
\newblock Publisher: Cambridge University Press (CUP).

\bibitem{fercoq_adaptive_2019}
{\sc O.~Fercoq and Z.~Qu}, {\em Adaptive restart of accelerated gradient
  methods under local quadratic growth condition}, IMA Journal of Numerical
  Analysis, 39 (2019), pp.~2069--2095.
\newblock arXiv:1709.02300 [math].

\bibitem{goldstein_field_2016}
{\sc T.~Goldstein, C.~Studer, and R.~Baraniuk}, {\em A field guide to
  forward-backward splitting with a {FASTA} implementation}, Dec. 2016.
\newblock arXiv:1411.3406 [cs].

\bibitem{jang_computer-assisted_2023}
{\sc U.~Jang, S.~D. Gupta, and E.~K. Ryu}, {\em Computer-assisted design of
  accelerated composite optimization methods: {OptISTA}}, May 2023.
\newblock arXiv:2305.15704 [math].

\bibitem{necoara_linear_2019}
{\sc I.~Necoara, Y.~Nesterov, and F.~Glineur}, {\em Linear convergence of first
  order methods for non-strongly convex optimization}, Mathematical
  Programming, 175 (2019), pp.~69--107.

\bibitem{nesterov_introductory_2004}
{\sc Y.~Nesterov}, {\em Introductory {Lectures} on {Convex} {Optimization}},
  vol.~87 of Applied {Optimization}, Springer US, Boston, MA, 2004.

\bibitem{nesterov_lecture_2018}
\leavevmode\vrule height 2pt depth -1.6pt width 23pt, {\em Lecture on {Convex}
  {Optimizations} {Chapter} 2, {Smooth} {Convex} {Optimization}}, in Lectures
  on {Convex} {Optimization}, Y.~Nesterov, ed., Springer {Optimization} and
  {Its} {Applications}, Springer International Publishing, Cham, 2018,
  pp.~59--137.

\bibitem{nesterov_lectures_2018}
\leavevmode\vrule height 2pt depth -1.6pt width 23pt, {\em Lectures on {Convex}
  {Optimization}}, vol.~137 of Springer {Optimization} and {Its}
  {Applications}, Springer International Publishing, Cham, 2018.

\bibitem{noel_nesterovs_nodate}
{\sc W.~Noel}, {\em Nesterov's method for convex optimization}, SIAM Review,
  65, pp.~539--562.

\bibitem{rudin_nonlinear_1992}
{\sc L.~I. Rudin, S.~Osher, and E.~Fatemi}, {\em Nonlinear total variation
  based noise removal algorithms}, Physica D: Nonlinear Phenomena, 60 (1992),
  pp.~259--268.

\bibitem{su_differential_2015}
{\sc W.~Su, S.~Boyd, and E.~J. Candes}, {\em A differential equation for
  modeling nesterov's accelerated gradient method: {Theory} and {Insights}},
  arXiv.org,  (2015).

\end{thebibliography}
